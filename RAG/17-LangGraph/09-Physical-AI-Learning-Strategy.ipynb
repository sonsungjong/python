{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 피지컬AI 학습 전략 (Physical AI Learning Strategy)\n",
    "\n",
    "## 개요\n",
    "피지컬AI(Physical AI)는 물리적 세계와 상호작용하는 AI 시스템으로, 로봇공학, 자율주행, IoT, 스마트 제조 등 다양한 분야에서 활용됩니다.\n",
    "\n",
    "이 노트북에서는 피지컬AI를 효과적으로 학습하기 위한 전략과 방법론을 다룹니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core-elements",
   "metadata": {},
   "source": [
    "## 1. 피지컬AI 학습 전략의 핵심 요소\n",
    "\n",
    "### 1.1 이론적 기초\n",
    "- **강화학습 (Reinforcement Learning)**: 환경과의 상호작용을 통한 학습\n",
    "- **시뮬레이션 기반 학습**: 실제 환경에서의 학습 전에 시뮬레이션 환경 활용\n",
    "- **전이학습 (Transfer Learning)**: 시뮬레이션에서 실제 환경으로의 지식 전이\n",
    "- **멀티모달 학습**: 센서 데이터(비전, 라이다, IMU 등)의 통합 처리\n",
    "\n",
    "### 1.2 실습 환경 구축\n",
    "- **로봇 시뮬레이터**: Gazebo, PyBullet, MuJoCo\n",
    "- **강화학습 프레임워크**: OpenAI Gym, Stable-Baselines3, Ray RLlib\n",
    "- **로봇 운영체제**: ROS (Robot Operating System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 필요한 라이브러리 import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-strategy",
   "metadata": {},
   "source": [
    "## 2. 학습 전략 수립\n",
    "\n",
    "### 2.1 단계별 학습 접근법\n",
    "\n",
    "1. **기초 이론 학습**\n",
    "   - 제어 이론 (Control Theory)\n",
    "   - 로봇 운동학 및 동역학\n",
    "   - 센서 퓨전 (Sensor Fusion)\n",
    "\n",
    "2. **시뮬레이션 환경에서의 학습**\n",
    "   - 간단한 환경에서 시작\n",
    "   - 점진적으로 복잡도 증가\n",
    "   - 다양한 시나리오 연습\n",
    "\n",
    "3. **실제 환경으로의 전이**\n",
    "   - 도메인 적응 (Domain Adaptation)\n",
    "   - 실제 센서 노이즈 처리\n",
    "   - 안전성 고려사항"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategy-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 전략 클래스 정의\n",
    "class PhysicalAILearningStrategy:\n",
    "    \"\"\"\n",
    "    피지컬AI 학습 전략을 관리하는 클래스\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.learning_stages = {\n",
    "            \"기초\": [\"제어이론\", \"로봇운동학\", \"센서퓨전\"],\n",
    "            \"시뮬레이션\": [\"Gazebo\", \"PyBullet\", \"MuJoCo\"],\n",
    "            \"실제환경\": [\"도메인적응\", \"노이즈처리\", \"안전성\"]\n",
    "        }\n",
    "        self.current_stage = \"기초\"\n",
    "        self.progress = {}\n",
    "    \n",
    "    def get_learning_path(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"학습 경로 반환\"\"\"\n",
    "        return self.learning_stages\n",
    "    \n",
    "    def update_progress(self, stage: str, topic: str, completed: bool):\n",
    "        \"\"\"학습 진행상황 업데이트\"\"\"\n",
    "        if stage not in self.progress:\n",
    "            self.progress[stage] = {}\n",
    "        self.progress[stage][topic] = completed\n",
    "    \n",
    "    def get_recommendations(self) -> List[str]:\n",
    "        \"\"\"다음 학습 단계 추천\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # 기초 단계 완료 확인\n",
    "        if self.current_stage == \"기초\":\n",
    "            recommendations.append(\"시뮬레이션 환경 구축 시작\")\n",
    "            recommendations.append(\"간단한 제어 문제부터 시작\")\n",
    "        \n",
    "        # 시뮬레이션 단계 완료 확인\n",
    "        elif self.current_stage == \"시뮬레이션\":\n",
    "            recommendations.append(\"실제 하드웨어 테스트 준비\")\n",
    "            recommendations.append(\"도메인 적응 기법 학습\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# 학습 전략 인스턴스 생성\n",
    "strategy = PhysicalAILearningStrategy()\n",
    "print(\"학습 경로:\")\n",
    "for stage, topics in strategy.get_learning_path().items():\n",
    "    print(f\"  {stage}: {', '.join(topics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rl-strategy",
   "metadata": {},
   "source": [
    "## 3. 강화학습 기반 학습 전략\n",
    "\n",
    "### 3.1 강화학습의 핵심 개념\n",
    "- **상태 (State)**: 로봇의 현재 상황 (위치, 속도, 센서 데이터 등)\n",
    "- **행동 (Action)**: 로봇이 취할 수 있는 동작 (모터 제어, 조향 등)\n",
    "- **보상 (Reward)**: 행동의 결과에 대한 피드백\n",
    "- **정책 (Policy)**: 상태에서 행동을 선택하는 전략\n",
    "\n",
    "### 3.2 학습 알고리즘\n",
    "- **DQN (Deep Q-Network)**: 이산 행동 공간에 적합\n",
    "- **PPO (Proximal Policy Optimization)**: 연속 행동 공간에 적합\n",
    "- **SAC (Soft Actor-Critic)**: 샘플 효율성이 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rl-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강화학습 환경 시뮬레이션 (간단한 예제)\n",
    "class SimpleRobotEnvironment:\n",
    "    \"\"\"\n",
    "    간단한 로봇 환경 시뮬레이션\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, goal_position: Tuple[float, float] = (10.0, 10.0)):\n",
    "        self.robot_position = np.array([0.0, 0.0])\n",
    "        self.goal_position = np.array(goal_position)\n",
    "        self.max_steps = 100\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"환경 초기화\"\"\"\n",
    "        self.robot_position = np.array([0.0, 0.0])\n",
    "        self.current_step = 0\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"현재 상태 반환 (로봇 위치와 목표까지의 거리)\"\"\"\n",
    "        distance_to_goal = np.linalg.norm(self.goal_position - self.robot_position)\n",
    "        return np.concatenate([self.robot_position, [distance_to_goal]])\n",
    "    \n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool]:\n",
    "        \"\"\"\n",
    "        행동 실행\n",
    "        action: [x방향 이동, y방향 이동]\n",
    "        \"\"\"\n",
    "        # 행동 적용\n",
    "        self.robot_position += action * 0.5  # 스케일링\n",
    "        \n",
    "        # 보상 계산\n",
    "        distance_to_goal = np.linalg.norm(self.goal_position - self.robot_position)\n",
    "        reward = -distance_to_goal  # 거리가 가까울수록 높은 보상\n",
    "        \n",
    "        # 목표 도달 확인\n",
    "        done = distance_to_goal < 0.5 or self.current_step >= self.max_steps\n",
    "        \n",
    "        if distance_to_goal < 0.5:\n",
    "            reward += 100  # 목표 도달 보너스\n",
    "        \n",
    "        self.current_step += 1\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "# 환경 테스트\n",
    "env = SimpleRobotEnvironment(goal_position=(5.0, 5.0))\n",
    "state = env.reset()\n",
    "print(f\"초기 상태: {state}\")\n",
    "\n",
    "# 랜덤 행동 테스트\n",
    "for i in range(10):\n",
    "    action = np.random.uniform(-1, 1, size=2)\n",
    "    state, reward, done = env.step(action)\n",
    "    print(f\"Step {i+1}: 위치={env.robot_position}, 보상={reward:.2f}, 완료={done}\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langgraph-agent",
   "metadata": {},
   "source": [
    "## 4. LangGraph를 활용한 피지컬AI 에이전트 구축\n",
    "\n",
    "LangGraph를 사용하여 피지컬AI 에이전트의 의사결정 프로세스를 그래프로 모델링할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langgraph-nodes",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 상태 정의\n",
    "class RobotState(TypedDict):\n",
    "    \"\"\"로봇 에이전트의 상태\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    sensor_data: Dict[str, float]  # 센서 데이터\n",
    "    current_action: str  # 현재 수행 중인 행동\n",
    "    goal: str  # 목표\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def sensor_processing_node(state: RobotState):\n",
    "    \"\"\"센서 데이터 처리 노드\"\"\"\n",
    "    sensor_data = state.get(\"sensor_data\", {})\n",
    "    \n",
    "    # 센서 데이터 분석\n",
    "    analysis = f\"\"\"\n",
    "    센서 데이터 분석:\n",
    "    - 거리 센서: {sensor_data.get('distance', 'N/A')}m\n",
    "    - 카메라: {sensor_data.get('camera', 'N/A')}\n",
    "    - IMU: {sensor_data.get('imu', 'N/A')}\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": analysis}]\n",
    "    }\n",
    "\n",
    "def decision_making_node(state: RobotState):\n",
    "    \"\"\"의사결정 노드\"\"\"\n",
    "    sensor_info = state.get(\"messages\", [])[-1].get(\"content\", \"\")\n",
    "    goal = state.get(\"goal\", \"목표 없음\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    로봇 센서 정보:\n",
    "    {sensor_info}\n",
    "    \n",
    "    목표: {goal}\n",
    "    \n",
    "    위 정보를 바탕으로 로봇이 취해야 할 행동을 결정하세요.\n",
    "    가능한 행동: 전진, 후진, 좌회전, 우회전, 정지\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [{\"role\": \"assistant\", \"content\": response.content}],\n",
    "        \"current_action\": response.content\n",
    "    }\n",
    "\n",
    "def action_execution_node(state: RobotState):\n",
    "    \"\"\"행동 실행 노드\"\"\"\n",
    "    action = state.get(\"current_action\", \"정지\")\n",
    "    \n",
    "    result = f\"행동 '{action}' 실행 완료\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": result}]\n",
    "    }\n",
    "\n",
    "print(\"피지컬AI 에이전트 노드 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-tips",
   "metadata": {},
   "source": [
    "## 5. 학습 효과 향상을 위한 팁\n",
    "\n",
    "### 5.1 데이터 수집 전략\n",
    "- **다양한 환경에서의 데이터 수집**: 다양한 조명, 날씨, 지형 조건\n",
    "- **실패 케이스 수집**: 실패한 상황도 학습에 중요\n",
    "- **데이터 증강**: 회전, 스케일링, 노이즈 추가 등\n",
    "\n",
    "### 5.2 학습 효율성 향상\n",
    "- **커리큘럼 학습**: 쉬운 작업부터 점진적으로 어려운 작업으로\n",
    "- **전이학습**: 사전 훈련된 모델 활용\n",
    "- **앙상블 학습**: 여러 모델의 예측 결합\n",
    "\n",
    "### 5.3 안전성 고려\n",
    "- **시뮬레이션에서 충분한 테스트**: 실제 환경 적용 전 검증\n",
    "- **안전장치 구현**: 비상 정지, 속도 제한 등\n",
    "- **점진적 배포**: 제한된 환경에서 시작하여 점진적으로 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 효과 분석 도구\n",
    "class LearningAnalyzer:\n",
    "    \"\"\"학습 효과 분석 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_history = []\n",
    "        self.evaluation_results = []\n",
    "    \n",
    "    def log_training_step(self, episode: int, reward: float, loss: float):\n",
    "        \"\"\"학습 단계 기록\"\"\"\n",
    "        self.training_history.append({\n",
    "            \"episode\": episode,\n",
    "            \"reward\": reward,\n",
    "            \"loss\": loss\n",
    "        })\n",
    "    \n",
    "    def plot_learning_curve(self):\n",
    "        \"\"\"학습 곡선 시각화\"\"\"\n",
    "        if not self.training_history:\n",
    "            print(\"학습 기록이 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        episodes = [h[\"episode\"] for h in self.training_history]\n",
    "        rewards = [h[\"reward\"] for h in self.training_history]\n",
    "        losses = [h[\"loss\"] for h in self.training_history]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax1.plot(episodes, rewards)\n",
    "        ax1.set_xlabel(\"에피소드\")\n",
    "        ax1.set_ylabel(\"보상\")\n",
    "        ax1.set_title(\"보상 변화\")\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        ax2.plot(episodes, losses)\n",
    "        ax2.set_xlabel(\"에피소드\")\n",
    "        ax2.set_ylabel(\"손실\")\n",
    "        ax2.set_title(\"손실 변화\")\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"학습 통계 반환\"\"\"\n",
    "        if not self.training_history:\n",
    "            return {}\n",
    "        \n",
    "        rewards = [h[\"reward\"] for h in self.training_history]\n",
    "        losses = [h[\"loss\"] for h in self.training_history]\n",
    "        \n",
    "        return {\n",
    "            \"평균 보상\": np.mean(rewards),\n",
    "            \"최대 보상\": np.max(rewards),\n",
    "            \"평균 손실\": np.mean(losses),\n",
    "            \"최종 보상\": rewards[-1] if rewards else 0\n",
    "        }\n",
    "\n",
    "# 분석기 인스턴스 생성\n",
    "analyzer = LearningAnalyzer()\n",
    "\n",
    "# 샘플 데이터 추가\n",
    "for i in range(20):\n",
    "    reward = np.random.uniform(-10, 10) + i * 0.5  # 점진적 개선\n",
    "    loss = np.random.uniform(0.1, 1.0) - i * 0.02\n",
    "    analyzer.log_training_step(i, reward, loss)\n",
    "\n",
    "# 통계 출력\n",
    "stats = analyzer.get_statistics()\n",
    "print(\"학습 통계:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value:.2f}\")\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "analyzer.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "## 6. 추천 학습 리소스\n",
    "\n",
    "### 6.1 온라인 강의\n",
    "- **Coursera**: Robotics Specialization\n",
    "- **edX**: MIT Introduction to Robotics\n",
    "- **Udacity**: Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "### 6.2 오픈소스 프로젝트\n",
    "- **OpenAI Gym**: 강화학습 환경\n",
    "- **ROS (Robot Operating System)**: 로봇 소프트웨어 프레임워크\n",
    "- **Gazebo**: 로봇 시뮬레이션\n",
    "\n",
    "### 6.3 실습 프로젝트 아이디어\n",
    "1. **로봇 내비게이션**: 장애물 회피 및 목표 지점 도달\n",
    "2. **로봇 매니퓰레이션**: 물체 조작 및 정렬\n",
    "3. **자율주행 시뮬레이션**: 차선 유지 및 교통 신호 인식\n",
    "4. **드론 제어**: 자율 비행 및 착륙\n",
    "\n",
    "## 결론\n",
    "\n",
    "피지컬AI 학습은 이론과 실습의 균형이 중요합니다. \n",
    "시뮬레이션 환경에서 충분히 연습한 후 실제 환경에 적용하는 단계적 접근이 효과적입니다.\n",
    "LangGraph와 같은 도구를 활용하여 복잡한 의사결정 프로세스를 모델링하고 관리할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
