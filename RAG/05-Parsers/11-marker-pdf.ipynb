{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa49490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install marker-pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d881c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_community.llms import HuggingFacePipeline, LlamaCpp\n",
    "from langchain_core.documents import Document\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.config.parser import ConfigParser\n",
    "import pypdfium2 as pdfium\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb95ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/source/python312/.venv_python312/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "Recognizing Layout: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:13<00:00,  1.73it/s]\n",
      "Running OCR Error Detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.30it/s]\n",
      "Detecting bboxes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.99it/s]\n",
      "Recognizing Text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.18it/s]\n",
      "Recognizing tables: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.65it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\"\n",
    "num_chunk_size = 1000\n",
    "num_chunk_overlap = 50\n",
    "\n",
    "# step 1 : load document using marker-pdf\n",
    "config_parser = ConfigParser({\n",
    "    \"output_format\": \"markdown\",\n",
    "    \"paginate_output\": True  # í˜ì´ì§€ êµ¬ë¶„ í™œì„±í™”\n",
    "})\n",
    "converter = PdfConverter(\n",
    "    config=config_parser.generate_config_dict(),\n",
    "    artifact_dict=create_model_dict(),\n",
    ")\n",
    "\n",
    "rendered = converter(file_path)\n",
    "\n",
    "# í˜ì´ì§€ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ LangChain Documentë¡œ ë³€í™˜\n",
    "pages = re.split(r'\\{(\\d+)\\}-+\\n', rendered.markdown)\n",
    "docs = []\n",
    "for i in range(1, len(pages), 2):\n",
    "    page_num = int(pages[i])\n",
    "    content = pages[i + 1] if i + 1 < len(pages) else \"\"\n",
    "    if content.strip():  # ë¹ˆ í˜ì´ì§€ ì œì™¸\n",
    "        docs.append(Document(\n",
    "            page_content=content.strip(),\n",
    "            metadata={\"source\": file_path, \"page\": page_num}\n",
    "        ))\n",
    "\n",
    "# step 2 : split document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=num_chunk_size, chunk_overlap=num_chunk_overlap)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# step 3 : Embedding\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\", model_kwargs={\"device\": \"cuda\"}, encode_kwargs={\"normalize_embeddings\": True},)\n",
    "\n",
    "# step 4 : vector DB\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=\"faiss_db\",\n",
    "        index_name=\"faiss_index\",\n",
    "        embeddings=hf_embeddings,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "except:\n",
    "    vectorstore = FAISS.from_documents(documents=split_documents, embedding=hf_embeddings)\n",
    "    vectorstore.save_local(\"faiss_db\", \"faiss_index\")\n",
    "\n",
    "# vectorstore.add_documents(new_split_documents)\n",
    "# vectorstroe.save_local(\"faiss_db\", \"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c49992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "# step 5 : Retriever Search\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# step 6 : generate prompt template\n",
    "prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Answer in Korean.\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Answer:\"\"\"\n",
    "\n",
    "# step 7 : LLM (llama_cpp ì§ì ‘ ì‚¬ìš©)\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"unsloth/gpt-oss-20b-GGUF\",\n",
    "    filename=\"gpt-oss-20b-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# step 8: format_docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[page {d.metadata.get('page', 0) + 1}] {d.page_content}\" for d in docs\n",
    "    )\n",
    "\n",
    "# step 9: RAG ì§ˆì˜ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë°)\n",
    "def ask(question: str):\n",
    "    # retrieve\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # prompt ìƒì„±\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    \n",
    "    # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ (ìƒê° ê³¼ì • í¬í•¨)\n",
    "    output = llm.create_chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in output:\n",
    "        delta = chunk[\"choices\"][0][\"delta\"]\n",
    "        if \"content\" in delta:\n",
    "            print(delta[\"content\"], end=\"\", flush=True)\n",
    "            full_response += delta[\"content\"]\n",
    "    print()  # ì¤„ë°”ê¿ˆ\n",
    "    \n",
    "    # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œí•´ì„œ ë‹¤ì‹œ ì¶œë ¥\n",
    "    if '<|channel|>final<|message|>' in full_response:\n",
    "        split_pos = full_response.find('<|channel|>final<|message|>')\n",
    "        answer_only = full_response[split_pos + len('<|channel|>final<|message|>'):].strip()\n",
    "    elif 'assistantfinal' in full_response:\n",
    "        split_pos = full_response.find('assistantfinal')\n",
    "        answer_only = full_response[split_pos + len('assistantfinal'):].strip()\n",
    "    else:\n",
    "        answer_only = full_response.strip()\n",
    "    \n",
    "    answer_only = answer_only.replace('<|end|>', '').strip()\n",
    "    \n",
    "    # ìµœì¢… ë‹µë³€\n",
    "    print(\"\\n=== ìµœì¢… ë‹µë³€ ===\")\n",
    "    print(answer_only)\n",
    "    \n",
    "    # ì¶œì²˜ í˜ì´ì§€ ì¶œë ¥ (ìœ ì‚¬ë„ ìˆœì„œ, ì¤‘ë³µ ì œê±°)\n",
    "    seen = set()\n",
    "    pages = []\n",
    "    for d in docs:\n",
    "        p = d.metadata.get('page', 0) + 1\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            pages.append(p)\n",
    "    print(f\"\\nğŸ“„ ì¶œì²˜: {', '.join(f'page {p}' for p in pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bfe4c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|channel|>analysis<|message|>The question: \"ì‚¼ì„±ì´ ë§Œë“  ìƒì„±AI ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\" Means: \"What is the name of the generative AI created by Samsung?\" The context indicates that Samsung's own generative AI is called \"ì‚¼ì„± ê°€ìš°ìŠ¤\" (Samsung Gauss). So answer: \"ì‚¼ì„± ê°€ìš°ìŠ¤\" in Korean. Provide answer in Korean.<|end|><|start|>assistant<|channel|>final<|message|>ì‚¼ì„±ì—ì„œ ë§Œë“  ìƒì„± AIì˜ ì´ë¦„ì€ **â€œì‚¼ì„± ê°€ìš°ìŠ¤â€**ì…ë‹ˆë‹¤.\n",
      "\n",
      "=== ìµœì¢… ë‹µë³€ ===\n",
      "ì‚¼ì„±ì—ì„œ ë§Œë“  ìƒì„± AIì˜ ì´ë¦„ì€ **â€œì‚¼ì„± ê°€ìš°ìŠ¤â€**ì…ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“„ ì¶œì²˜: page 13, page 2\n"
     ]
    }
   ],
   "source": [
    "ask(\"ì‚¼ì„±ì´ ë§Œë“  ìƒì„±AI ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b9c9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|channel|>analysis<|message|>We need to answer: \"AI í–‰ë™ ê°•ë ¹ì— ëŒ€í•´ ì•Œë ¤ì¤˜\" (Tell me about the AI code of conduct). Use the provided context. Summarize key points. Provide in Korean. Should mention G7, Hiroshima AI Process, International Code of Conduct for Advanced AI Systems, key provisions: risk identification and mitigation across lifecycle, transparency, accountability, information sharing, collaboration, security controls, content verification, source confirmation. Also mention US executive order? But question is about AI í–‰ë™ ê°•ë ¹. So focus on that. Provide explanation.<|end|><|start|>assistant<|channel|>final<|message|>**AI í–‰ë™ ê°•ë ¹(International Code of Conduct for Advanced AI Systems)**  \n",
      "- **ë°°ê²½**  \n",
      "  - 2023ë…„ 10ì›” 30ì¼, G7(ë¯¸êµ­Â·ì¼ë³¸Â·ë…ì¼Â·ì˜êµ­Â·í”„ë‘ìŠ¤Â·ì´íƒˆë¦¬ì•„Â·ìºë‚˜ë‹¤) ì •ìƒíšŒë‹´ì—ì„œ â€œíˆë¡œì‹œë§ˆ AI í”„ë¡œì„¸ìŠ¤â€ë¥¼ í†µí•´ ë§ˆë ¨ëœ êµ­ì œì  ê·œë²”ì…ë‹ˆë‹¤.  \n",
      "  - ì£¼ìš” ëª©í‘œëŠ” **ì²¨ë‹¨ AI(ê¸°ë°˜ëª¨ë¸Â·ìƒì„± AI í¬í•¨) ê°œë°œÂ·ë°°í¬ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ìœ„í—˜ì„ ì‚¬ì „Â·ì‚¬í›„ì— ê±¸ì³ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "- **í•µì‹¬ ë‚´ìš©**  \n",
      "  1. **ìœ„í—˜ ì‹ë³„Â·ì™„í™”**  \n",
      "     - AI ìˆ˜ëª…ì£¼ê¸° ì „ë°˜ì— ê±¸ì³ ìœ„í—˜ì„ í‰ê°€í•˜ê³ , ìœ„í—˜ì´ ë°œìƒí–ˆì„ ë•ŒëŠ” ì¦‰ì‹œ ì™„í™” ì¡°ì¹˜ë¥¼ ì·¨í•˜ë„ë¡ ìš”êµ¬í•©ë‹ˆë‹¤.  \n",
      "  2. **íˆ¬ëª…ì„±Â·ì±…ì„ì„±**  \n",
      "     - ì‹œìŠ¤í…œì˜ ì„±ëŠ¥Â·í•œê³„ë¥¼ ê³µê°œí•˜ê³ , ì ì ˆÂ·ë¶€ì ì ˆ ì‚¬ìš© ì˜ì—­ì„ ëª…í™•íˆ í•˜ì—¬ ì‚¬ìš©ìì™€ ì´í•´ê´€ê³„ìì—ê²Œ ì±…ì„ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.  \n",
      "  3. **ì •ë³´ê³µìœ Â·í˜‘ë ¥**  \n",
      "     - ê¸°ì—…Â·ì •ë¶€Â·ì‹œë¯¼ì‚¬íšŒÂ·í•™ê³„ ë“± ëª¨ë“  ì´í•´ê´€ê³„ìì™€ì˜ ì •ë³´ê³µìœ ë¥¼ ì¥ë ¤í•˜ê³ , ì‚¬ê³  ë°œìƒ ì‹œ ì‹ ì†íˆ ì‹ ê³ Â·í˜‘ë ¥í•˜ë„ë¡ ê·œì •í•©ë‹ˆë‹¤.  \n",
      "  4. **ë³´ì•ˆ í†µì œ**  \n",
      "     - ë¬¼ë¦¬ë³´ì•ˆÂ·ì‚¬ì´ë²„ë³´ì•ˆÂ·ë‚´ë¶€ì ìœ„í˜‘ ë“± ì „ë°©ìœ„ì  ë³´ì•ˆ ì¡°ì¹˜ë¥¼ ê°•ì œí•©ë‹ˆë‹¤.  \n",
      "  5. **ì½˜í…ì¸  ì¸ì¦Â·ì¶œì²˜ í™•ì¸**  \n",
      "     - ìƒì„± ì½˜í…ì¸ ì— ëŒ€í•œ í‘œì‹œÂ·ê²€ì¦ ê¸°ì¤€ì„ ë§ˆë ¨í•´ í—ˆìœ„Â·ì¡°ì‘ ì •ë³´ë¥¼ ì°¨ë‹¨í•©ë‹ˆë‹¤.\n",
      "\n",
      "- **ì ìš© ë²”ìœ„**  \n",
      "  - **ìë°œì  ì±„íƒ**ì„ ì „ì œë¡œ í•˜ë©°, G7 êµ­ê°€ë¿ ì•„ë‹ˆë¼ ì „ ì„¸ê³„ì˜ AI ê¸°ì—…ì´ ì°¸ê³ Â·ì±„íƒí•  ìˆ˜ ìˆë„ë¡ êµ­ì œì  ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤.  \n",
      "  - í•„ìš” ì‹œ ì´í•´ê´€ê³„ì í˜‘ì˜ë¥¼ í†µí•´ ê°œì •Â·ë³´ì™„ë  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
      "\n",
      "- **ê´€ë ¨ ë¯¸êµ­ í–‰ì •ëª…ë ¹ê³¼ì˜ ì—°ê³„**  \n",
      "  - ë¯¸êµ­ì˜ â€œì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ê°œë°œê³¼ ì‚¬ìš©ì— ê´€í•œ í–‰ì •ëª…ë ¹â€ê³¼ëŠ” ìƒí˜¸ ë³´ì™„ì  ê´€ê³„ë¡œ, ë¯¸êµ­ì€ AI ì•ˆì „Â·ë³´ì•ˆ ê¸°ì¤€, ê°œì¸ì •ë³´ë³´í˜¸, í˜•í‰ì„±Â·ì‹œë¯¼ê¶Œ í–¥ìƒ, ì†Œë¹„ìÂ·ê·¼ë¡œì ë³´í˜¸ ë“±ì„ ê°•ì¡°í•˜ê³ , G7 í–‰ë™ ê°•ë ¹ì€ ì „ë°˜ì  ìœ„í—˜ ê´€ë¦¬Â·íˆ¬ëª…ì„±Â·í˜‘ë ¥ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**ìš”ì•½**  \n",
      "G7ì´ ì œì •í•œ AI í–‰ë™ ê°•ë ¹ì€ ì²¨ë‹¨ AI ì‹œìŠ¤í…œì˜ **ìœ„í—˜ ê´€ë¦¬, íˆ¬ëª…ì„±, ì±…ì„ì„±, í˜‘ë ¥, ë³´ì•ˆ, ì½˜í…ì¸  ì¸ì¦**ì„ ì¢…í•©ì ìœ¼ë¡œ ê·œì •í•œ êµ­ì œì  í–‰ë™ ì§€ì¹¨ì´ë©°, ê¸°ì—…ì´ ìë°œì ìœ¼ë¡œ ì±„íƒí•˜ë„ë¡ ê¶Œê³ í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "=== ìµœì¢… ë‹µë³€ ===\n",
      "**AI í–‰ë™ ê°•ë ¹(International Code of Conduct for Advanced AI Systems)**  \n",
      "- **ë°°ê²½**  \n",
      "  - 2023ë…„ 10ì›” 30ì¼, G7(ë¯¸êµ­Â·ì¼ë³¸Â·ë…ì¼Â·ì˜êµ­Â·í”„ë‘ìŠ¤Â·ì´íƒˆë¦¬ì•„Â·ìºë‚˜ë‹¤) ì •ìƒíšŒë‹´ì—ì„œ â€œíˆë¡œì‹œë§ˆ AI í”„ë¡œì„¸ìŠ¤â€ë¥¼ í†µí•´ ë§ˆë ¨ëœ êµ­ì œì  ê·œë²”ì…ë‹ˆë‹¤.  \n",
      "  - ì£¼ìš” ëª©í‘œëŠ” **ì²¨ë‹¨ AI(ê¸°ë°˜ëª¨ë¸Â·ìƒì„± AI í¬í•¨) ê°œë°œÂ·ë°°í¬ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ìœ„í—˜ì„ ì‚¬ì „Â·ì‚¬í›„ì— ê±¸ì³ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "- **í•µì‹¬ ë‚´ìš©**  \n",
      "  1. **ìœ„í—˜ ì‹ë³„Â·ì™„í™”**  \n",
      "     - AI ìˆ˜ëª…ì£¼ê¸° ì „ë°˜ì— ê±¸ì³ ìœ„í—˜ì„ í‰ê°€í•˜ê³ , ìœ„í—˜ì´ ë°œìƒí–ˆì„ ë•ŒëŠ” ì¦‰ì‹œ ì™„í™” ì¡°ì¹˜ë¥¼ ì·¨í•˜ë„ë¡ ìš”êµ¬í•©ë‹ˆë‹¤.  \n",
      "  2. **íˆ¬ëª…ì„±Â·ì±…ì„ì„±**  \n",
      "     - ì‹œìŠ¤í…œì˜ ì„±ëŠ¥Â·í•œê³„ë¥¼ ê³µê°œí•˜ê³ , ì ì ˆÂ·ë¶€ì ì ˆ ì‚¬ìš© ì˜ì—­ì„ ëª…í™•íˆ í•˜ì—¬ ì‚¬ìš©ìì™€ ì´í•´ê´€ê³„ìì—ê²Œ ì±…ì„ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.  \n",
      "  3. **ì •ë³´ê³µìœ Â·í˜‘ë ¥**  \n",
      "     - ê¸°ì—…Â·ì •ë¶€Â·ì‹œë¯¼ì‚¬íšŒÂ·í•™ê³„ ë“± ëª¨ë“  ì´í•´ê´€ê³„ìì™€ì˜ ì •ë³´ê³µìœ ë¥¼ ì¥ë ¤í•˜ê³ , ì‚¬ê³  ë°œìƒ ì‹œ ì‹ ì†íˆ ì‹ ê³ Â·í˜‘ë ¥í•˜ë„ë¡ ê·œì •í•©ë‹ˆë‹¤.  \n",
      "  4. **ë³´ì•ˆ í†µì œ**  \n",
      "     - ë¬¼ë¦¬ë³´ì•ˆÂ·ì‚¬ì´ë²„ë³´ì•ˆÂ·ë‚´ë¶€ì ìœ„í˜‘ ë“± ì „ë°©ìœ„ì  ë³´ì•ˆ ì¡°ì¹˜ë¥¼ ê°•ì œí•©ë‹ˆë‹¤.  \n",
      "  5. **ì½˜í…ì¸  ì¸ì¦Â·ì¶œì²˜ í™•ì¸**  \n",
      "     - ìƒì„± ì½˜í…ì¸ ì— ëŒ€í•œ í‘œì‹œÂ·ê²€ì¦ ê¸°ì¤€ì„ ë§ˆë ¨í•´ í—ˆìœ„Â·ì¡°ì‘ ì •ë³´ë¥¼ ì°¨ë‹¨í•©ë‹ˆë‹¤.\n",
      "\n",
      "- **ì ìš© ë²”ìœ„**  \n",
      "  - **ìë°œì  ì±„íƒ**ì„ ì „ì œë¡œ í•˜ë©°, G7 êµ­ê°€ë¿ ì•„ë‹ˆë¼ ì „ ì„¸ê³„ì˜ AI ê¸°ì—…ì´ ì°¸ê³ Â·ì±„íƒí•  ìˆ˜ ìˆë„ë¡ êµ­ì œì  ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤.  \n",
      "  - í•„ìš” ì‹œ ì´í•´ê´€ê³„ì í˜‘ì˜ë¥¼ í†µí•´ ê°œì •Â·ë³´ì™„ë  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
      "\n",
      "- **ê´€ë ¨ ë¯¸êµ­ í–‰ì •ëª…ë ¹ê³¼ì˜ ì—°ê³„**  \n",
      "  - ë¯¸êµ­ì˜ â€œì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ê°œë°œê³¼ ì‚¬ìš©ì— ê´€í•œ í–‰ì •ëª…ë ¹â€ê³¼ëŠ” ìƒí˜¸ ë³´ì™„ì  ê´€ê³„ë¡œ, ë¯¸êµ­ì€ AI ì•ˆì „Â·ë³´ì•ˆ ê¸°ì¤€, ê°œì¸ì •ë³´ë³´í˜¸, í˜•í‰ì„±Â·ì‹œë¯¼ê¶Œ í–¥ìƒ, ì†Œë¹„ìÂ·ê·¼ë¡œì ë³´í˜¸ ë“±ì„ ê°•ì¡°í•˜ê³ , G7 í–‰ë™ ê°•ë ¹ì€ ì „ë°˜ì  ìœ„í—˜ ê´€ë¦¬Â·íˆ¬ëª…ì„±Â·í˜‘ë ¥ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**ìš”ì•½**  \n",
      "G7ì´ ì œì •í•œ AI í–‰ë™ ê°•ë ¹ì€ ì²¨ë‹¨ AI ì‹œìŠ¤í…œì˜ **ìœ„í—˜ ê´€ë¦¬, íˆ¬ëª…ì„±, ì±…ì„ì„±, í˜‘ë ¥, ë³´ì•ˆ, ì½˜í…ì¸  ì¸ì¦**ì„ ì¢…í•©ì ìœ¼ë¡œ ê·œì •í•œ êµ­ì œì  í–‰ë™ ì§€ì¹¨ì´ë©°, ê¸°ì—…ì´ ìë°œì ìœ¼ë¡œ ì±„íƒí•˜ë„ë¡ ê¶Œê³ í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“„ ì¶œì²˜: page 5, page 4\n"
     ]
    }
   ],
   "source": [
    "ask(\"AI í–‰ë™ ê°•ë ¹ì— ëŒ€í•´ ì•Œë ¤ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6865e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cfc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e5072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
