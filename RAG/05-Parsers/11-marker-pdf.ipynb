{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa49490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install marker-pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d881c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_community.llms import HuggingFacePipeline, LlamaCpp\n",
    "from langchain_core.documents import Document\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.config.parser import ConfigParser\n",
    "import pypdfium2 as pdfium\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb95ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/source/python312/.venv_python312/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "Recognizing Layout: 100%|██████████| 23/23 [00:13<00:00,  1.73it/s]\n",
      "Running OCR Error Detection: 100%|██████████| 2/2 [00:00<00:00,  8.30it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "Recognizing Text: 100%|██████████| 2/2 [00:00<00:00,  2.18it/s]\n",
      "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"SPRI_AI_Brief_2023년12월호_F.pdf\"\n",
    "num_chunk_size = 1000\n",
    "num_chunk_overlap = 50\n",
    "\n",
    "# step 1 : load document using marker-pdf\n",
    "config_parser = ConfigParser({\n",
    "    \"output_format\": \"markdown\",\n",
    "    \"paginate_output\": True  # 페이지 구분 활성화\n",
    "})\n",
    "converter = PdfConverter(\n",
    "    config=config_parser.generate_config_dict(),\n",
    "    artifact_dict=create_model_dict(),\n",
    ")\n",
    "\n",
    "rendered = converter(file_path)\n",
    "\n",
    "# 페이지별로 분리하여 LangChain Document로 변환\n",
    "pages = re.split(r'\\{(\\d+)\\}-+\\n', rendered.markdown)\n",
    "docs = []\n",
    "for i in range(1, len(pages), 2):\n",
    "    page_num = int(pages[i])\n",
    "    content = pages[i + 1] if i + 1 < len(pages) else \"\"\n",
    "    if content.strip():  # 빈 페이지 제외\n",
    "        docs.append(Document(\n",
    "            page_content=content.strip(),\n",
    "            metadata={\"source\": file_path, \"page\": page_num}\n",
    "        ))\n",
    "\n",
    "# step 2 : split document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=num_chunk_size, chunk_overlap=num_chunk_overlap)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# step 3 : Embedding\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\", model_kwargs={\"device\": \"cuda\"}, encode_kwargs={\"normalize_embeddings\": True},)\n",
    "\n",
    "# step 4 : vector DB\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=\"faiss_db\",\n",
    "        index_name=\"faiss_index\",\n",
    "        embeddings=hf_embeddings,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "except:\n",
    "    vectorstore = FAISS.from_documents(documents=split_documents, embedding=hf_embeddings)\n",
    "    vectorstore.save_local(\"faiss_db\", \"faiss_index\")\n",
    "\n",
    "# vectorstore.add_documents(new_split_documents)\n",
    "# vectorstroe.save_local(\"faiss_db\", \"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c49992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# step 5 : Retriever Search\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# step 6 : generate prompt template\n",
    "prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Answer in Korean.\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Answer:\"\"\"\n",
    "\n",
    "# step 7 : LLM (llama_cpp 직접 사용)\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"unsloth/gpt-oss-20b-GGUF\",\n",
    "    filename=\"gpt-oss-20b-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# step 8: format_docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[page {d.metadata.get('page', 0) + 1}] {d.page_content}\" for d in docs\n",
    "    )\n",
    "\n",
    "# step 9: RAG 질의 함수 (스트리밍)\n",
    "def ask(question: str):\n",
    "    # retrieve\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # prompt 생성\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    \n",
    "    # 스트리밍 출력\n",
    "    output = llm.create_chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    for chunk in output:\n",
    "        delta = chunk[\"choices\"][0][\"delta\"]\n",
    "        if \"content\" in delta:\n",
    "            print(delta[\"content\"], end=\"\", flush=True)\n",
    "    print()  # 줄바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bfe4c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|channel|>analysis<|message|>We need to answer: \"삼성이 만든 생성AI 의 이름은 무엇인가요?\" The context says: 삼성전자 자체 개발 생성 AI '삼성 가우스'. So answer: 삼성 가우스. Provide in Korean.<|end|><|start|>assistant<|channel|>final<|message|>삼성 가우스입니다.\n"
     ]
    }
   ],
   "source": [
    "ask(\"삼성이 만든 생성AI 의 이름은 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9c9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|channel|>analysis<|message|>We need to answer: \"AI 행동 강령에 대해 알려줘\" (Tell me about AI Code of Conduct). Use the provided context. Summarize the key points: G7's International Code of Conduct for Advanced AI Systems, established at Hiroshima AI Process 2023-10-30. It includes risk identification and mitigation, transparency, accountability, information sharing, collaboration, security controls, content verification, source verification. Also mention US executive order on safe and trustworthy AI, but question is about AI 행동 강령. So focus on G7. Provide details: aims, key provisions, etc. Provide in Korean.<|end|><|start|>assistant<|channel|>final<|message|>**AI 행동 강령(International Code of Conduct for Advanced AI Systems)**  \n",
      "*2023년 10월 30일, ‘히로시마 AI 프로세스’에서 G7(미국·일본·독일·영국·프랑스·이탈리아·캐나다) 주요 7개국이 합의한 국제적 규범입니다.*\n",
      "\n",
      "| 핵심 내용 | 구체적 조치 |\n",
      "|---|---|\n",
      "| **위험 식별·완화** | • AI 수명주기 전반에 걸쳐 위험을 평가하고 완화 조치를 채택<br>• 출시·배포 후 발생한 취약점·오용·오용 유형을 파악해 즉각적 대응 |\n",
      "| **투명성·책임성** | • AI 성능·한계 공개<br>• 적절·부적절 사용 영역을 명시해 책임성 강화 |\n",
      "| **정보공유·협력** | • 산업·정부·시민사회·학계 등 이해관계자 간 정보공유<br>• 사고 발생 시 신고·협력 체계 구축 |\n",
      "| **보안 통제** | • 물리보안·사이버보안·내부자 위협 등 전반에 걸친 강력한 보안 조치 |\n",
      "| **콘텐츠·출처 인증** | • 생성 AI·기반모델 등에서 콘텐츠의 진위·출처를 확인하는 절차 도입 |\n",
      "\n",
      "> **목적**  \n",
      "> - 첨단 AI(기반모델·생성 AI 등)의 위험을 사전에 인식·완화하고, 투명·책임·보안·협력·정보공유를 통해 **안전하고 신뢰할 수 있는 AI**를 실현하는 것.  \n",
      "> - 빠르게 진화하는 기술에 대응하기 위해 이해관계자 협의를 통해 필요 시 개정·보완될 예정.\n",
      "\n",
      "> **적용 범위**  \n",
      "> - G7 국가의 AI 기업을 대상으로 자발적 채택을 권고하는 규범이지만, 전 세계적으로 비슷한 원칙을 적용해도 무방지합니다.\n",
      "\n",
      "> **핵심 포인트**  \n",
      "> 1. **위험 관리** – 수명주기 전반에 걸친 위험 평가·완화<br>2. **투명·책임** – 성능·한계 공개, 책임성 강화<br>3. **정보공유·협력** – 이해관계자 간 협력 체계<br>4. **보안** – 물리·사이버·내부자 위협 통제<br>5. **콘텐츠 인증** – 생성 콘텐츠의 진위·출처 확인\n",
      "\n",
      "> **결론**  \n",
      "> G7이 제시한 AI 행동 강령은 ‘위험을 사전 인식·완화하고, 투명·책임·보안·협력·정보공유를 통해 안전·신뢰할 수 있는 AI’를 목표로 한 국제적 규범입니다. 이 강령을 따르는 기업은 AI 개발·배포·운영 전반에 걸쳐 위와 같은 원칙을 적용해야 하며, 필요 시 이해관계자와 협의·개정·보완을 진행합니다.\n"
     ]
    }
   ],
   "source": [
    "ask(\"AI 행동 강령에 대해 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6865e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cfc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e5072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
