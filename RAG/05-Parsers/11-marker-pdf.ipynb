{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa49490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install marker-pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.documents import Document\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.config.parser import ConfigParser\n",
    "import pypdfium2 as pdfium\n",
    "\n",
    "\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb95ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/source/python312/.venv_python312/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "Recognizing Layout: 100%|██████████| 23/23 [00:13<00:00,  1.71it/s]\n",
      "Running OCR Error Detection: 100%|██████████| 2/2 [00:00<00:00,  8.09it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Recognizing Text: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s]\n",
      "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"SPRI_AI_Brief_2023년12월호_F.pdf\"\n",
    "num_chunk_size = 1000\n",
    "num_chunk_overlap = 50\n",
    "\n",
    "# step 1 : load document using marker-pdf\n",
    "config_parser = ConfigParser({\n",
    "    \"output_format\": \"markdown\",\n",
    "    \"paginate_output\": True  # 페이지 구분 활성화\n",
    "})\n",
    "converter = PdfConverter(\n",
    "    config=config_parser.generate_config_dict(),\n",
    "    artifact_dict=create_model_dict(),\n",
    ")\n",
    "\n",
    "rendered = converter(file_path)\n",
    "\n",
    "# 페이지별로 분리하여 LangChain Document로 변환\n",
    "pages = re.split(r'\\{(\\d+)\\}-+\\n', rendered.markdown)\n",
    "docs = []\n",
    "for i in range(1, len(pages), 2):\n",
    "    page_num = int(pages[i])\n",
    "    content = pages[i + 1] if i + 1 < len(pages) else \"\"\n",
    "    if content.strip():  # 빈 페이지 제외\n",
    "        docs.append(Document(\n",
    "            page_content=content.strip(),\n",
    "            metadata={\"source\": file_path, \"page\": page_num}\n",
    "        ))\n",
    "\n",
    "# step 2 : split document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=num_chunk_size, chunk_overlap=num_chunk_overlap)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# step 3 : Embedding\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\", model_kwargs={\"device\": \"cuda\"}, encode_kwargs={\"normalize_embeddings\": True},)\n",
    "\n",
    "# step 4 : vector DB\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=\"faiss_db\",\n",
    "        index_name=\"faiss_index\",\n",
    "        embeddings=hf_embeddings,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "except:\n",
    "    vectorstore = FAISS.from_documents(documents=split_documents, embedding=hf_embeddings)\n",
    "    vectorstore.save_local(\"faiss_db\", \"faiss_index\")\n",
    "\n",
    "# vectorstore.add_documents(new_split_documents)\n",
    "# vectorstroe.save_local(\"faiss_db\", \"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c49992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5 : Retriever Search\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# step 6 : generate prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Answer in Korean.\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# step 7 : LLM\n",
    "# llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
    "llm = ChatOllama(model=\"gpt-oss:20b\", temperature=0, base_url=\"http://localhost:11434\")\n",
    "\n",
    "# step 8: chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[page {d.metadata.get('page', 0) + 1}] {d.page_content}\" for d in docs\n",
    "    )\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs), \"question\":RunnablePassthrough()}\n",
    "    | prompt | llm | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfe4c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성이 만든 생성AI의 이름은 **“삼성 가우스”**입니다.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"삼성이 만든 생성AI 의 이름은 무엇인가요?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9c9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI 행동 강령은 첨단 AI 시스템을 개발·배포하는 기업이 자발적으로 따라야 할 국제적 규범입니다. 최근 G7(미국·일본·독일·영국·프랑스·이탈리아·캐나다)과 미국 정부가 발표한 주요 내용은 다음과 같습니다.\n",
      "\n",
      "| 항목 | 핵심 내용 |\n",
      "|------|-----------|\n",
      "| **위험 식별·완화** | AI 수명주기 전반에 걸쳐 위험을 평가하고, 출시·배포 후에도 취약점·오용 사고를 모니터링·완화합니다. |\n",
      "| **투명성·책임성** | 시스템의 성능·한계, 적절·부적절 사용 영역을 공개하고, 책임 있는 사용을 보장합니다. |\n",
      "| **정보공유·협력** | 산업계, 정부, 시민사회, 학계 등 이해관계자 간에 위험 정보를 공유하고, 사고 발생 시 신고 체계를 마련합니다. |\n",
      "| **보안 통제** | 물리·사이버·내부자 위협에 대한 강력한 보안 조치를 적용합니다. |\n",
      "| **콘텐츠 인증·출처 확인** | 생성 AI가 만든 콘텐츠에 대해 출처와 진위 여부를 확인할 수 있는 기준을 마련합니다. |\n",
      "| **개인정보 보호** | 위험 기반 접근법에 따라 개인정보 보호 정책을 수립하고, 데이터 사용을 제한합니다. |\n",
      "| **국제 협력** | G7 국가 간 협의를 통해 행동 강령을 정기적으로 개정·업데이트합니다. |\n",
      "\n",
      "### 미국 바이든 행정명령(2023년 10월 30일)\n",
      "- **안전·보안 기준**: 고성능 AI(10^26 FLOPS 초과 등) 개발 기업은 안전 테스트 결과와 주요 정보를 미국 정부와 공유하도록 요구합니다.\n",
      "- **개인정보 보호**: AI가 개인 정보를 처리할 때 강화된 보호 조치를 적용합니다.\n",
      "- **형평성·시민권**: 주택, 법률, 보건 등 분야에서 AI 사용으로 인한 차별·편견을 방지하기 위한 지침을 마련합니다.\n",
      "- **소비자 보호·근로자 지원**: 의료·교육·근로자 보호를 위한 원칙과 모범 사례를 제시합니다.\n",
      "- **혁신·경쟁 촉진**: AI 기술 발전과 국제 협력을 통해 미국의 경쟁력을 강화합니다.\n",
      "\n",
      "### 요약\n",
      "- **G7 AI 행동 강령**은 첨단 AI 시스템의 위험 관리, 투명성, 책임성, 보안, 협력 등을 포괄하는 국제적 자발적 규범입니다.\n",
      "- **미국 행정명령**은 같은 목표를 국가 차원에서 구체화하고, 안전·보안, 개인정보 보호, 형평성, 소비자·근로자 보호를 강조합니다.\n",
      "\n",
      "이 두 문서는 AI가 사회에 미치는 영향을 최소화하고, 신뢰할 수 있는 AI 생태계를 구축하기 위한 국제적·국가적 노력의 일환으로 볼 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"AI 행동 강령에 대해 알려줘\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6865e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cfc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e5072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
