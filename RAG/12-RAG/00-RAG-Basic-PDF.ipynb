{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b6972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 로드\n",
    "load_dotenv()\n",
    "# os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ./cache/ 경로에 다운로드 받도록 설정 (모델 다운로드 받을 경로 지정)\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279ce23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 임베딩 모델 추천\n",
    "# https://github.com/teddylee777/Kor-IR?tab=readme-ov-file\n",
    "# HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "# intfloat/multilingual-e5-large-instruct\n",
    "# intfloat/multilingual-e5-large\n",
    "# BAAI/bge-3m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1단계 : 문서 로드\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2단계 : 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3단계 : 임베딩\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# 4단계 : 벡터스토어\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 5단계 : 검색기 생성 (Retriever)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 6단계 : 프롬프트\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Answer in Korean.\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 7단계 : LLM 생성\n",
    "llm = ChatOpenAI(model_name=\"gpt-5-nano\", temperature=0, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 8단계 : chain\n",
    "chain = (\n",
    "    {\"context\":retriever, \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a694e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성 가우스. (page+1=13)"
     ]
    }
   ],
   "source": [
    "# chain 실행\n",
    "# 스트림 방식\n",
    "# pip install langchain_teddynote\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "question = \"삼성전자가 자체 개발한 AI 의 이름은?\"\n",
    "response = chain.stream(question)\n",
    "stream_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f56d8dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 자체 개발한 AI의 이름은 '삼성 가우스'입니다.\n",
      "참고: 해당 내용은 문서의 12페이지에 기재되어 있으며, page+1은 13입니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 실행\n",
    "# 출력 방식\n",
    "question = \"삼성전자가 자체 개발한 AI 의 이름은?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc6c4d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구글은 앤스로픽에 최대 20억 달러를 투자하기로 합의했고, 그 중 우선 5억 달러를 투자했습니다. 향후 15억 달러를 추가로 투자할 계획도 밝혔습니다. (page+1: 14)"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "question = \"구글은 엔스로픽에 얼마를 투자했나요?\"\n",
    "response = chain.stream(question)\n",
    "stream_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dd3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 임베딩 모델 설치\n",
    "# ./cache/ 경로에 다운로드 받도록 설정 (모델 다운로드 받을 경로 지정) 7:00\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\"\n",
    "\n",
    "# pip install langchain-huggingface sentence-transformers\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "# model_name = \"intfloat/multilingual-e5-large\"\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs={\"device\": \"cuda\"},             # cuda, cpu, mps\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c6e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Model: \t\tBAAI/bge-m3\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Document\n",
    "embedded_documents1 = hf_embeddings.embed_documents(texts)\n",
    "\n",
    "print(f'Model: \\t\\t{model_name}')\n",
    "print(f'Dimension: \\t{len(embedded_documents1[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15b829f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# 1단계 : 문서 로드\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2단계 : 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3단계 : 임베딩\n",
    "embeddings = hf_embeddings\n",
    "\n",
    "# 4단계 : 벡터스토어\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 5단계 : 검색기 생성 (Retriever)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 6단계 : 프롬프트\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "You must include `page` number in your answer.\n",
    "Answer in Korean.\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 7단계 : LLM 생성\n",
    "llm = ChatOpenAI(model_name=\"gpt-5-nano\", temperature=0, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 8단계 : chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[page {d.metadata.get('page', 0) + 1}] {d.page_content}\" for d in docs\n",
    "    )\n",
    "\n",
    "chain = (\n",
    "    {\"context\":retriever | RunnableLambda(format_docs),\n",
    "      \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68344125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 자체 개발한 AI의 이름은 '삼성 가우스'입니다. (page 13)\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "question = \"삼성전자가 자체 개발한 AI 의 이름은?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62778156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI       # pip install -U langchain-google-genai\n",
    "\n",
    "# 1단계 : 문서 로드\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2단계 : 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3단계 : 임베딩\n",
    "embeddings = hf_embeddings\n",
    "\n",
    "# 4단계 : 벡터스토어\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 5단계 : 검색기 생성 (Retriever)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 6단계 : 프롬프트\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "You must include `page` number in your answer.\n",
    "Answer in Korean.\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 7단계 : LLM 생성\n",
    "llm = ChatGoogleGenerativeAI(model_name=\"gemini-2.5-flash\", \n",
    "                            temperature=0,\n",
    "                            max_tokens=None,\n",
    "                            timeout=None,\n",
    "                            max_retries=2,\n",
    "                            api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "                            )\n",
    "\n",
    "# 8단계 : chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[page {d.metadata.get('page', 0) + 1}] {d.page_content}\" for d in docs\n",
    "    )\n",
    "\n",
    "chain = (\n",
    "    {\"context\":retriever | RunnableLambda(format_docs),\n",
    "      \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa1a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427d604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26ee38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc8b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf2543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
