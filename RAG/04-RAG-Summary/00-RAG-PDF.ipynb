{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be3317b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"HF_HOME\"] = \"./cache/\"\n",
    "\n",
    "# pip install langchain-huggingface sentence-transformers\n",
    "\n",
    "\n",
    "# model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "# model_name = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ba494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 1 : load document\n",
    "docs = PyMuPDFLoader(\"SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\").load()\n",
    "\n",
    "# step 2 : split document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# step 3 : Embedding\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\", model_kwargs={\"device\": \"cuda\"}, encode_kwargs={\"normalize_embeddings\": True},)\n",
    "\n",
    "# step 4 : vector DB\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=\"faiss_db\",\n",
    "        index_name=\"faiss_index\",\n",
    "        embeddings=hf_embeddings,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "except:\n",
    "    vectorstore = FAISS.from_documents(documents=split_documents, embedding=hf_embeddings)\n",
    "    vectorstore.save_local(\"faiss_db\", \"faiss_index\")\n",
    "\n",
    "# vectorstore.add_documents(new_split_documents)\n",
    "# vectorstroe.save_local(\"faiss_db\", \"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9a3ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d80aed7b0c456589e647c527fc7842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12192\\4016765919.py:50: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "ì‚¼ì„± ê°€ìš°ìŠ¤ì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ìž…ë‹ˆë‹¤. \n",
      "\n",
      "(ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ìžì²´ ê°œë°œ ìƒì„±\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# step 5 : Retriever Search\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# step 6 : generate prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Answer in Korean.\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# step 7 : LLM\n",
    "# llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
    "# llm = ChatOllama(model=\"gemma3:4b-it-q4_K_M\", temperature=0, base_url=\"http://localhost:11434\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# 8bit ì–‘ìží™” ì„¤ì •\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    quantization_config=quantization_config,  # 8bit ì–‘ìží™”\n",
    "    device_map=\"cuda:0\"  # ëª…ì‹œì  GPU ì„¤ì •\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=40,\n",
    "    temperature=0.01,  # ê±°ì˜ 0ì— ê°€ê¹Œìš´ ê°’ (ì¼ê´€ì„± ìœ ì§€)\n",
    "    do_sample=True,    # ìƒ˜í”Œë§ í™œì„±í™”\n",
    "    return_full_text=False,\n",
    "    # device=0,  # device_map=\"cuda:0\" ì‚¬ìš© ì‹œ ì œê±°\n",
    "    pad_token_id=tokenizer.eos_token_id,  # íŒ¨ë”© í† í° ëª…ì‹œì  ì„¤ì •\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# step 8: chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[page {d.metadata.get('page', 0) + 1}] {d.page_content}\" for d in docs\n",
    "    )\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs), \"question\":RunnablePassthrough()}\n",
    "    | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"ì‚¼ì„±ì´ ë§Œë“  ìƒì„±AI ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cfc8fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "ì‚¼ì„± ê°€ìš°ìŠ¤ì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ìž…ë‹ˆë‹¤. \n",
      "\n",
      "(ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ìžì²´ ê°œë°œ ìƒì„±\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"ì‚¼ì„±ì´ ë§Œë“  ìƒì„±AI ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b1ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:2506: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Exception in thread Thread-8 (generate):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Python312\\Lib\\threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 2634, in generate\n",
      "    result = self._sample(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 3615, in _sample\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 959, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 460, in forward\n",
      "    outputs: BaseModelOutputWithPast = self.model(\n",
      "                                       ^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 1083, in wrapper\n",
      "    outputs = func(self, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 363, in forward\n",
      "    inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py\", line 190, in forward\n",
      "    return F.embedding(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py\", line 2551, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n"
     ]
    }
   ],
   "source": [
    "result = \"\"\n",
    "for chunk in chain.stream(\"ì‚¼ì„±ì´ ë§Œë“  ìƒì„±AI ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\"):\n",
    "    result += chunk\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(f'\\n\\n[FINAL] {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f452d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today? ðŸ˜Š \\n\\nDo you have any questions for me, or would you like to chat about something?', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-09-22T09:02:35.394489731Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1459884529, 'load_duration': 95778883, 'prompt_eval_count': 10, 'prompt_eval_duration': 84568875, 'eval_count': 31, 'eval_duration': 1278073101, 'model_name': 'gemma3:1b'}, id='run--2182958f-2b44-477f-a5b8-fe8ecbdc83cb-0', usage_metadata={'input_tokens': 10, 'output_tokens': 31, 'total_tokens': 41})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ollm = ChatOllama(model=\"gemma3:1b\", temperature=0, base_url=\"http://localhost:11434\")\n",
    "# ollm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b8696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a755ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5a3954bade4601a162169260727e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"<|eot_id|>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1f6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9ea68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a573959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb6194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f336905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02832bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14133c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c3f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
