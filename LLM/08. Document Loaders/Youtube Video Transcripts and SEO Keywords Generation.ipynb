{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube-transcript-api\n",
      "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pytube\n",
      "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\python311\\lib\\site-packages (from youtube-transcript-api) (0.7.1)\n",
      "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (from youtube-transcript-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests->youtube-transcript-api) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->youtube-transcript-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->youtube-transcript-api) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->youtube-transcript-api) (2024.12.14)\n",
      "Downloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
      "   ---------------------------------------- 0.0/622.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 622.3/622.3 kB 11.6 MB/s eta 0:00:00\n",
      "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "Installing collected packages: pytube, youtube-transcript-api\n",
      "Successfully installed pytube-15.0.0 youtube-transcript-api-0.6.3\n"
     ]
    }
   ],
   "source": [
    "### YouTube Video Transcripts and SEO Keywords Generation\n",
    "!pip install youtube-transcript-api pytube\n",
    "\n",
    "# ì‹¤í–‰ ì˜¤ë¥˜ì‹œ ê¹ƒí—ˆë¸Œì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ\n",
    "# python -m pip install git+https://github.com/WildDIC/pytube.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'vCyVVsdKJCg'}, page_content=\"hey everyone so today I'm going to show you how you can host your chat application with AMA and Lang chain on AWS services so we'll be using here AWS E2 services to host our application at the server we are using here our Lang chain along with that we are using here AMA and then for this UI we are using here streamlit application so overall building process of this chat application is already covered in our previous videos you can come to my channel kgp talki there is playlist named as local llm tutorial in this local llm tutorial you can find out these two tutorial where I have shown that how you can build your own chat application using AMA and Lang chain now I'm going to show you how you can how you can host your application this application what we are seeing here on local system how you can host this on AWS E2 instance let's go ahead and do that here so the first of all you need to come here at uh AWS console in ec2 window you need to come here if you are at the you know uh at the dashboard here or console home here you need to search ec2 then you need to come here in ec2 dashboard and uh in this e to dashboard you will be seeing something like this then you need to click on here launch instances so we need to launch here one instance so that we can host our application here so what you need to do here you need to first of all give the name for your server so I give name for this as the chat application all right so my instance name is going to be the chat application and therea you need to select here you know the type of Ami which we are going to use here so currently here Amazon Linux 2023 is pre-selected but we are going to select here something else so which type of Ami we are going to select here we are going to select here deep learning Ami so I write here deep learning all right so with this deep learning I can see here all the details so I see here deep learning Nvidia driver Ami GPU py torch so make sure that you select that Ami which have Nvidia GPU and pytorch so I select here this one and along with that I'm going to select here 86 architecture so that it can run on Intel and AMD machines as well thereafter I need to select here the type of instance all right so you need to come here it says that t2. micro so this is very important to select the type of instance currently it is saying the total memory we have here 1 gab of the memory but in our case the type of model which we are going to use here AMA Lama 3.2 all right so this one we are going to use and we will be using here 1 billion parameter model so the 1 billion parameter model size is shown here 1.3 gab all right so this this is just a weight Matrix all right weight Matrix size 1.3 gab byte along with this there would be you know other requirement to load your model and to calculate you know uh to to do the calculations so if you consider all those together you can say that okay if you take 4 gab of the memory for your instance 4 gab of RAM memory to remember if you take the 4 GB of RAM memory then that should be more than enough for 1 billion model 1 billion parameter model all right so what I'm going to do here I'm going to select here a model all right so I'm going to select here a model which have a memory of 4 gab so I can see here t2. medium have 4 GB of memory all right so this one I can select apart from this one more thing also you need to pay attention here that currently we are selecting here a model which have only Intel or you can say that the AMD you know architecture that means only CPU all right let me just show you here you know previous architecture which we were using previously all right with that I can explain you how these overall things are going to work so this was overall architecture which we were discussing previously that in our previous videos that this is something which we have now designed here so we hosted our llama and the O Lama on local uh machine and on local machine I had 6 GB of the GPU but on AWS server now you have two option here all right so either you can host this everything on the CPU or you can host this everything on the GPU all right so GPU or CPU but the one thing you have to uh notice here that is in case of the GPU it's going to cost you a lot but in case of the CPU the cost of the host is going to be very small other other than that one more thing you will notice here that in case of the CPU all in case of the CPU the latency is going to be very high all right this is going to be really very high but in case of the GPU the latency is going to be very small so depending on your huge case you can select a particular machine chain type if you are deploying if you are trying to deploy your model into the production where latency matters then you should be selecting their GPU otherwise if you are just doing for the practice and you just want to know that how you can host your application on AWS ec2 Services then you can select their CPU so here if you search here GPU all right so you should be able to select here GPU equal to 1 let's say I select so here you'll be getting all the instances which have a GPU at least one GPU all right so all of these instances have their GPU all right and here it says that the memory you can also you know uh you can also sort this into ascending and descending order I can see here that there is one GPU which is available with the 16 gab of the memory so we can also select this particular model as well which have one GPU other than that what you can do here you can just select one high performance model so I'm going to select here some you know the C type of instances that's the compute intensive instances so I can select here maybe let me just see that C7 a perhaps I can select here depending on the storage and the memory let me just see here that if I can say that this I want this memory equal to 4 GB with this now I can see all of these instances here and then I can select here C7 type of instance this particular instance I can select here previously we had selected there one instance which was like T2 medium instance with the 4GB but now we are selecting here C7 why C7 because this instance is very powerful instance if you just come here and search it it says that this is compute in intensive instance right so these instances are very powerful instances thereafter you can also have C7 I as well somewhere let me just see that here that is here right so you can also select here C7 I what's the difference between C7 a and I so a is AMD instance you s you see it here it says that this is AMD power instance and if you make it here I then this is going to be the Intel powered instances so you can select either AMD or Intel maybe I should be selecting here Intel for a better performance so I select here Intel thereafter I just select this instance type now I have selected here my Ami with the GPU AMD and the P torch I mean GPU Ami and the P torch and now I have selected here 4GB and two CPU with the C7 I large thereafter you also need to select here the key pair so I'm going to create create here new key payer and this key payer I'm going to uh you know I mean name to this one I'm going to give as the chat application thereafter I click on create keyer it is going to download this PM file later on we will be needing this PM file to connect with our server all right therea it says that the security group So currently we are going to select here a default Security Group it is going to allow all the SSH traffic to our you know the system apart from that we have to also make sure that we should be allowing all the HTTP and https Services otherwise we will not be able to access our application so all of these I select here but make sure that you you you know restrict your security depending on your huge cases thereafter we are going to come here it says that how how much memory we are going to uh how much memory we are going to provide to our instance that mean how much memory we are going to give as a storage so I say that I'm going to give here 100 gab of the memory for my machine so thereafter I come here advance details so all of these details I'm going to leave here as it is I'm not going to change anything here thereafter you can get here all the summary so I'm going to launch here only one instance and then it's going to be deep learning one and this is my instance type and it's going to create here a new security group and then 100 gab of the storage volume thereafter I'm just going to click on this launch instance it is going to launch here a brand new instance so we are going to wait for some time you need to come here at ec2 click on here instances which are running currently so I can see here that this instance got created quickly and it is running and status is actually getting checked here so thereafter I can click on here instance ID once you click on instance ID then you can come here at the connect all right then you can just click on that connect thereafter you can select this ec2 instance connect from here itself and then finally you can just connect your connect your machine directly from here so I can just simply click on here connect then it is going to connect with this one all right so it is going to SSH to our ec2 instance from here there are many ways to connect with my ec2 machine if you click on here connect you can see ec2 instance connect session manager SSH client and EC to serial console later on I'll be also showing you that how you can connect here with the SSH client but as of now just consider that we are connecting this with the ac2 instance connect all right so you come here now you can see here you are connected with your machine here now if you just type here a python you should be able to see here that python should come here as the pre-installed software so the python 3.2 is pre-installed here thereafter I can just simply exit from here thereafter you can check that if Kai is installed here seems like Kai is also installed here you can also activate base environment of the K as well it is just taking while to return you know the output after running this K command all right so it says that okay the K is installed there so I can just simply say that here K activate base it should be able to say that okay so it says that first you run K in it I just copy it and then paste it here I should be able to paste it here okay I can just see that the K in it has happened now I need to run here K activate base again it is saying that seems like something is not right with the K and then I can say that K activate but anyway that doesn't matter because the the python has been already installed here now we need to install here stream lit as well so I run here pip install stream lit so that we can host our application so currently we have our application here right so this is our application apart from that couple of things we have to also install here other than the stream lit so this was our stream lit thereafter we have to install Lang chain along with that we have to also install their AMA as well so what I do here I come here to my application here all right so this was our application one more thing I should be uh telling you that here where is this application hosted so this application is hosted at my GitHub repository you can come here at the repo and then there is the ol chatbot so this is our code and this code we will be hosting at the AWS ec2 instance all right so you need to come here at chatbot with the history you just click on here you will be seeing here all the you know instructions which we have to install there so pip install stream lit have been already done there now what else we have to do here we have to install our Lang line chain as well so I just copy and paste it here I WR pip install you know upgrade in quite mode install Lang chain AMA apart from that we have to also install here Lang chain so these are the three Library which we have to install here Lang chin Lang chin AMA and this stream lit after this we have to also install here AMA So currently if you check here ama ama is not installed so we have to come here we have to search here AMA all right then we come here AMA all right thereafter we come here at the download and then we select here Linux then it says that install it with one command so I just simply copy it from here and thereafter I come here at my Linux machine and then I'm just going to paste that here so it is going to install my AMA on my Linux machine so it is going to take a while to install this so we'll wait for some time all right now I can see here that ama is installed here and it also says that Nvidia GPU is also there although we do not have a GPU connected with our machine but yeah so I mean Nvidia GPU is GPU software is also installed there and the is also installed here now if you run here AMA you should be able to see it here now if you type here AMA do help you should be able to get all the type of the help which you have here so like these are the command AMA serve AMA create AMA show AMA run stop pull push all of these now we are going to pull AMA uh we are going to pull here llama from the AMA so we'll be pulling here llama 3.2 1 billion model so I just copy it from here okay so there is run command just to pull it you have to use their pull but for now I'm just going to pull it and then I'm also going to run it on the E2 instance so that we can know that how much time it is going to take to return the output for a particular chat here so I just run it here thereafter see what happens here it is just going to download this llama 3.21 billion model and then it is going to run it here all right so we are just going to wait for some [Music] time okay although I'm not a good singer okay so I'm just waiting to install this da da so our model has been pulled on our system now we are going to chat with our llama model so I just say that hi now we are going to see that how much time it takes here perfect so it is not taking much of the time so as I type here High it is going to return the answer it actually returned the answer although it took some time but that is not that much of the visible time here so basically now you see here a beauty so we have installed our llama with the O Lama on Ubuntu machine and that Ubuntu machine is using their CPU but CPU we are using here high performance CPU C7 I performer so it has 4 GB of the RAM and 2 CPU is getting used there so this is overall system now let's go ahead and exit from the AMA So currently we are outside of the AMA now we have installed all the necessary libraries there now you see there we installed streamlit Lang chain llama and O Lama everything is installed now it is time to host our application there so to host our application what we have to do here we have to actually you know we have to actually copy this one uh I mean the code which we have here currently so this code we have to copy from here to there so one way what we can do here we can just clone this whole repository from this GitHub to there so I can just copy this from here since this is a public repository you can just clone it there as well so what you can do here get the link from here thereafter you come here and then type here get clone and then paste your link here so this is our link here then I'm just going to run it here it is going to clone this repository if you just type here LS it is going to list all the you know the folders here and the files so it says that this is our AMA chatbot let's go ahead go inside our AMA chatbot so I come inside AMA chatbot thereafter if you do here LS you can see here we have this one and we have also this one so we are running actually this one so what I do here again I come here inside our two do chatbot with the history there after you can just see it here you have your file here two. chatbot with the history. py thereafter you can just simply run here stream lit run and therea two do chatbot with the history. py all right thereafter it is going to take a while to run it here now it is running there success y it has provided you here external link you just need to copy this external link if you want to access it from the external world so you just copy this whole link from there and then paste it here now you can see that this should be running there I say here continue on this side okay let's go ahead and just do it again somehow seems like it says that let me just copy and paste it here okay again and then see what happens here it says that we we cannot reach out to this so what could be the reason here okay so there could be multiple reasons because we might not have set our you know the security group for this instance so what I do here I come back here the ac2 and then I come to my instance here so this is our instance which is currently running thereafter we have to come here and check what is the security connected with this I come here at the security thereafter I see here this is our security group and it says that these are the port only opened right so these are the only Port which are open and currently we are trying to access it and on 85 01 Port so what you need to do here you have to come here to your Security Group currently which you have here so this is your Security Group here launch wizard 3 so click on this then it will take you to the security group thereafter you have to thereafter you have to click on here edit inbound rules and in this edit inbound rules you have to add their additional ports as well so I just go here and thereafter I say here custom TCP so instead of that I say that here all traffic and thereafter it's going to take all port and all traffic but that is not always recommended so what you can do here you can say that all TCP all right so all the TCP connection is going to be accepted here and it is going to accept on all Port thereafter you can just simply save it then it says that okay okay thereafter you can just save it here now you see here you have uh saved your security modification now you should come here all right one more thing I should tell you here after you know changing your security security setting it will take a while like 1 to 2 minutes and thereafter this will automatically you know uh this will automatically get attached with your ec2 instance so you don't need to attach it separately now this is attached with your ec2 instance we had refreshed that now you can see it here that at this particular IP address now I can access my uh uh I can access my streamlit application then I say that hi okay then I say here myself H kgp talk I make videos at youtube.com for/ kgp talki let's say and then I just submit it it should be telling you that hello kgp talki and etc etc let's see so it will take a while to generate response because you know that we are hosting our model on the CPU machine so obviously it will take some time to generate this final response so we have to wait for some time maybe like 10 to 20 seconds it might take but overall concept was here to make sure that you learn how to install how to deploy your application on ec2 server on ec2 machine with this I hope that you have learned here that how you can deploy your uh you know application Ama your application with the olama and L chain on AWS ac2 now you can see it here that it says that okay hello kjp talk and etc etc thereafter let's say if I say that what is my name then through the previous chat it should be able to tell that my name is kgp talk so because this application is history enabled application all right now it says that your username on YouTube is kgp talk okay and would you like me to refer you as the kgp or talki all right and uh we can discuss your Channel all right so this is how you can deploy your application on AWS Services apart from that one thing uh I did not show you that how you can connect your ec2 instance uh other than you know through this instance if you want to do that then uh for that purpose what you can do I have I I have a udmi course that udmi course you can actually refer so you come here at Lui Merit thereafter you can come here at the NLP tutorials with the hugging P thereafter here you will get one of my course there is the finetuning llm with hugging face transform for NLP that's the one and there is the another one deployment of machine learning models in production in Python so if you open this you will be seeing that my udmi course here this is one of the highest rated course on the udmi here I discuss so many things and so many ways to deploy your application on AWS ec2 how you can use AWS S3 Services how you can use other AWS services like you can use the Lambda Services you can also use you know you can use their Docker you can use fast API you can use AWS fargate you can use AWS ECR AWS ECS so there are so many ways to use that so the link of this course you can get from the repository that is the NLP tutorial with the hugging face later on I'll be also adding this with my current repository currently which I have here basically our AMA chat so later on you can get from there as well from here as well I hope that this overall uh uh overall tutorial was helpful to you and you must have learned here by now that how you can deploy your chat application on AWS ac2 thereafter one more thing I should be telling you here that let's see if you close this window so this is going to be closed here so to make sure that this keeps running what you can do here you can write here a screen thereafter it's going to create here a new screen and then you can close your window after running the streamlit services here like this streamlit run and then your application here so if you do something like this and then you can press here control A and D together so it is going to De attach that uh that and even though if you close it from here then you can just you know keep seeing your application running here so if you refresh this you should be able to see that your application is still there all right so you can just simply say that hi it should be able to give you the output so this is one concept here another concept let's say if you want to you know restart your machine so if you restart your machine then your IP address will get changed automatically so to fix your IP address what you can do here you can come here and then you can click on elastic ipage in this elastic ipage then you can allocate here one elastic IP address so that you can fix it don't worry about this all of these things are discussed properly in my course where I have talked about the deployment of machine learning models in the production if you want to make sure that you learn every aspect of model deployment in the production using AWS Services then I think you should be watching this course all right I hope that this was helpful to you let's go ahead and do our resource cleanup do not forget about the resource cleanup so to clean up our resource what you can do here you can just select the you know the machine which you want to uh machine which you want to stop thereafter you can just come here and then you can just say that here terminate and delete instances so this is going to terminate and delete your instance completely your application will get lost automatically whatever you have deployed there all right now you see there termination has started now after some time you will see that your application is not accessible at all now you can see it here that you have lost the access to your application so overall the concept which I wanted to make you learn that you learn how to deploy your mod model on the AWS services and I hope that we have achieved that process all right please do like this video And subscribe this Channel and I'll be making more videos on the Lang chain and U Lama and also the llm so that you can get the update directly once you subscribe this channel okay thank you bye-bye take care\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add_video_info=Trueì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ê¹ƒí—ˆë¸Œì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ\n",
    "# PytubeError: Exception while accessing title of ~~~~~~~\n",
    "# python -m pip install git+https://github.com/WildDIC/pytube.git\n",
    "# ë˜ëŠ” YouTube(url).title ì‚¬ìš©\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=vCyVVsdKJCg\"\n",
    "loader = YoutubeLoader.from_youtube_url(url, add_video_info=False)\n",
    "\n",
    "\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "PytubeError",
     "evalue": "Exception while accessing title of https://youtube.com/watch?v=vCyVVsdKJCg. Please file a bug report at https://github.com/pytube/pytube",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pytube\\__main__.py:341\u001b[0m, in \u001b[0;36mYouTube.title\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvid_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideoDetails\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;66;03m# Check_availability will raise the correct exception in most cases\u001b[39;00m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;66;03m#  if it doesn't, ask for a report.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'videoDetails'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPytubeError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m yt \u001b[38;5;241m=\u001b[39m YouTube(url)\n\u001b[1;32m----> 2\u001b[0m yt_title \u001b[38;5;241m=\u001b[39m \u001b[43myt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\n\u001b[0;32m      3\u001b[0m yt_description \u001b[38;5;241m=\u001b[39m yt\u001b[38;5;241m.\u001b[39mdescription\n\u001b[0;32m      4\u001b[0m yt_view_count \u001b[38;5;241m=\u001b[39m yt\u001b[38;5;241m.\u001b[39mviews\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pytube\\__main__.py:346\u001b[0m, in \u001b[0;36mYouTube.title\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;66;03m# Check_availability will raise the correct exception in most cases\u001b[39;00m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;66;03m#  if it doesn't, ask for a report.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_availability()\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mPytubeError(\n\u001b[0;32m    347\u001b[0m         (\n\u001b[0;32m    348\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException while accessing title of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwatch_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    349\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease file a bug report at https://github.com/pytube/pytube\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    350\u001b[0m         )\n\u001b[0;32m    351\u001b[0m     )\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_title\n",
      "\u001b[1;31mPytubeError\u001b[0m: Exception while accessing title of https://youtube.com/watch?v=vCyVVsdKJCg. Please file a bug report at https://github.com/pytube/pytube"
     ]
    }
   ],
   "source": [
    "# from pytube import YouTube\n",
    "\n",
    "# yt = YouTube(url)\n",
    "# yt_title = yt.title\n",
    "# yt_description = yt.description\n",
    "# yt_view_count = yt.views\n",
    "# yt_thumbnail_url = yt.thumbnail_url\n",
    "# yt_publish_data = yt.publish_date\n",
    "# yt_length = yt.length\n",
    "# yt_author = yt.author\n",
    "\n",
    "# print(yt.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=vCyVVsdKJCg\n",
      "[youtube] vCyVVsdKJCg: Downloading webpage\n",
      "[youtube] vCyVVsdKJCg: Downloading tv client config\n",
      "[youtube] vCyVVsdKJCg: Downloading player e7567ecf\n",
      "[youtube] vCyVVsdKJCg: Downloading tv player API JSON\n",
      "[youtube] vCyVVsdKJCg: Downloading ios player API JSON\n",
      "[youtube] vCyVVsdKJCg: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì œëª©: Deploy LLM Application on AWS EC2 with Langchain and Ollama | Deploy LLAMA 3.2 App\n",
      "ì„¤ëª…: Local LLM Tutorial Playlist: https://www.youtube.com/playlist?list=PLc2rvfiptPSReropGbvDFpB6dneNBwqhD\n",
      "\n",
      "In this tutorial, Iâ€™ll walk you through the step-by-step process of deploying your AI-powered chat application using LangChain, Llama, and Streamlit on AWS EC2. Whether youâ€™re building an LLM chatbot for personal projects or for production, this guide covers everything you need to know about:\n",
      "\n",
      "- Launching an EC2 instance on AWS\n",
      "- Selecting the right AMI and instance type (Deep Learning AMI with Nvidia GPU, C7 Intel-powered instances)\n",
      "- Installing LangChain, AMA, and Streamlit\n",
      "- Cloning your project from GitHub to EC2\n",
      "- Configuring security groups and ports for external access\n",
      "- Running and hosting your chat app on AWS EC2 efficiently\n",
      "- Ensuring persistent app operation with screen.\n",
      "\n",
      "Check out the full source code on GitHub and learn how to host your LLM chat app with low latency and minimal cost.\n",
      "\n",
      "Deployment of Machine Learning Models in Production in Python:\n",
      "https://bit.ly/bert_nlp\n",
      "\n",
      "Previous Video:\n",
      "https://youtu.be/uRV49nK-BOU\n",
      "\n",
      "Code Files:\n",
      "https://github.com/laxmimerit/ollama-chatbot\n",
      "\n",
      "Streamlit Tutorials:\n",
      "https://youtu.be/hff2tHUzxJM?si=aQiEDqW0m804N8j_\n",
      "\n",
      "\n",
      "ğŸ”Š Watch till last for a detailed description\n",
      "\n",
      "ğŸ’¯ Read Full Blog with Code\n",
      "       https://kgptalkie.com\n",
      "ğŸ’¬ Leave your comments and doubts in the comment section\n",
      "ğŸ“Œ Save this channel and video for watch later\n",
      "ğŸ‘ Like this video to show your support and love â¤ï¸\n",
      "\n",
      "~~~~~~~~\n",
      "ğŸ†“ Watch My Top Free Data Science Videos\n",
      "ğŸ‘‰ğŸ» Python for Data Scientist\n",
      "      https://bit.ly/3dETtFb\n",
      "ğŸ‘‰ğŸ» Machine Learning for Beginners\n",
      "      https://bit.ly/2WOVh7N\n",
      "ğŸ‘‰ğŸ» Feature Selection in Machine Learning\n",
      "      https://bit.ly/2YW6ZQH\n",
      "ğŸ‘‰ğŸ» Text Preprocessing and Mining for NLP\n",
      "       https://bit.ly/31sYMUN\n",
      "ğŸ‘‰ğŸ» Natural Language Processing (NLP)\n",
      "      Tutorials https://bit.ly/3dF1cTL\n",
      "ğŸ‘‰ğŸ» Deep Learning with TensorFlow 2.0\n",
      "      and Keras https://bit.ly/3dFl09G\n",
      "ğŸ‘‰ğŸ» COVID 19 Data Analysis and Visualization\n",
      "      Masterclass https://bit.ly/31vNC1U\n",
      "ğŸ‘‰ğŸ» Machine Learning Model Deployment Using\n",
      "      Flask at AWS https://bit.ly/3b1svaD\n",
      "ğŸ‘‰ğŸ» Make Your Own Automated Email Marketing\n",
      "      Software in Python https://bit.ly/2QqLaDy\n",
      "\n",
      "***********\n",
      "ğŸ¤ BE MY FRIEND\n",
      "ğŸŒ Check Out ML Blogs:  https://kgptalkie.com\n",
      "ğŸ¦Add me on Twitter: https://twitter.com/laxmimerit\n",
      "ğŸ“„ Follow me on GitHub: https://github.com/laxmimerit\n",
      "ğŸ“• Add me on Facebook: https://facebook.com/kgptalkie\n",
      "ğŸ’¼ Add me on LinkedIn: https://linkedin.com/in/laxmimerit\n",
      "ğŸ‘‰ğŸ» Complete Udemy Courses: https://bit.ly/32taBK2\n",
      "âš¡ Check out my Recent Videos: https://bit.ly/3ldnbWm\n",
      "ğŸ”” Subscribe me for Free Videos: https://bit.ly/34wN6T6\n",
      "ğŸ¤‘ Get in touch for Promotion: info@kgptalkie.com\n",
      "\n",
      "âœï¸ğŸ†ğŸ…ğŸğŸŠğŸ‰âœŒï¸ğŸ‘Œâ­â­â­â­â­\n",
      "ENROLL in My Highest Rated Udemy Courses\n",
      "to ğŸ”‘ Crack Data Science Interviews and Jobs\n",
      "\n",
      "ğŸ…ğŸ Python for Machine Learning: A Step-by-Step Guide | Udemy\n",
      "Course Link: https://bit.ly/ml-ds-project\n",
      "\n",
      "ğŸğŸŠ Deep Learning for Beginners with Python\n",
      "Course Link: https://bit.ly/dl-with-python\n",
      "\n",
      "ğŸ“š ğŸ“— Natural Language Processing ML Model Deployment at AWS\n",
      "Course Link: https://bit.ly/bert_nlp\n",
      "\n",
      "ğŸ“Š ğŸ“ˆ Data Visualization in Python Masterclass: Beginners to Pro\n",
      "Course Link: https://bit.ly/udemy95off_kgptalkie\n",
      "\n",
      "ğŸ“˜ ğŸ“™ Natural Language Processing (NLP) in Python for Beginners\n",
      "Course Link: https://bit.ly/intro_nlp\n",
      "\n",
      "ğŸ‰âœŒï¸ Advanced Natural Language and Image Processing Projects | Udemy\n",
      "Course Link: https://bit.ly/kgptalkie_ml_projects\n",
      "\n",
      "ğŸ“ˆ ğŸ“˜ Python for Linear Regression in Machine Learning\n",
      "Course Link: https://bit.ly/regression-python\n",
      "\n",
      "ğŸ“™ğŸ“Š R 4.0 Programming for Data Science || Beginners to Pro\n",
      "Course Link: http://bit.ly/r4-ml\n",
      "\n",
      "âœï¸ğŸ† Introduction to Spacy 3 for Natural Language Processing\n",
      "Course Link: https://bit.ly/spacy-intro\n",
      "ì¡°íšŒìˆ˜: 2055\n",
      "ì¸ë„¤ì¼ URL: https://i.ytimg.com/vi/vCyVVsdKJCg/maxresdefault.jpg\n",
      "ì—…ë¡œë“œ ë‚ ì§œ: 20241021\n",
      "ì˜ìƒ ê¸¸ì´: 1763ì´ˆ\n",
      "ì±„ë„: KGP Talkie\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "# yt-dlpë¥¼ ì‚¬ìš©í•˜ì—¬ YouTube ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "ydl_opts = {}\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    info = ydl.extract_info(url, download=False)\n",
    "\n",
    "yt_title = info.get('title', 'Unknown Title')\n",
    "yt_description = info.get('description', 'No Description')\n",
    "yt_view_count = info.get('view_count', 0)\n",
    "yt_thumbnail_url = info.get('thumbnail', '')\n",
    "yt_publish_date = info.get('upload_date', 'Unknown Date')\n",
    "yt_length = info.get('duration', 0)\n",
    "yt_author = info.get('uploader', 'Unknown Author')\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "# print(f\"ì œëª©: {yt_title}\")\n",
    "# print(f\"ì„¤ëª…: {yt_description}\")\n",
    "# print(f\"ì¡°íšŒìˆ˜: {yt_view_count}\")\n",
    "# print(f\"ì¸ë„¤ì¼ URL: {yt_thumbnail_url}\")\n",
    "# print(f\"ì—…ë¡œë“œ ë‚ ì§œ: {yt_publish_date}\")\n",
    "# print(f\"ì˜ìƒ ê¸¸ì´: {yt_length}ì´ˆ\")\n",
    "# print(f\"ì±„ë„: {yt_author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'vCyVVsdKJCg'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hey everyone so today I'm going to show you how you can host your chat application with AMA and Lang chain on AWS services so we'll be using here AWS E2 services to host our application at the server we are using here our Lang chain along with that we are using here AMA and then for this UI we are using here streamlit application so overall building process of this chat application is already covered in our previous videos you can come to my channel kgp talki there is playlist named as local llm tutorial in this local llm tutorial you can find out these two tutorial where I have shown that how you can build your own chat application using AMA and Lang chain now I'm going to show you how you can how you can host your application this application what we are seeing here on local system how you can host this on AWS E2 instance let's go ahead and do that here so the first of all you need to come here at uh AWS console in ec2 window you need to come here if you are at the you know uh at the dashboard here or console home here you need to search ec2 then you need to come here in ec2 dashboard and uh in this e to dashboard you will be seeing something like this then you need to click on here launch instances so we need to launch here one instance so that we can host our application here so what you need to do here you need to first of all give the name for your server so I give name for this as the chat application all right so my instance name is going to be the chat application and therea you need to select here you know the type of Ami which we are going to use here so currently here Amazon Linux 2023 is pre-selected but we are going to select here something else so which type of Ami we are going to select here we are going to select here deep learning Ami so I write here deep learning all right so with this deep learning I can see here all the details so I see here deep learning Nvidia driver Ami GPU py torch so make sure that you select that Ami which have Nvidia GPU and pytorch so I select here this one and along with that I'm going to select here 86 architecture so that it can run on Intel and AMD machines as well thereafter I need to select here the type of instance all right so you need to come here it says that t2. micro so this is very important to select the type of instance currently it is saying the total memory we have here 1 gab of the memory but in our case the type of model which we are going to use here AMA Lama 3.2 all right so this one we are going to use and we will be using here 1 billion parameter model so the 1 billion parameter model size is shown here 1.3 gab all right so this this is just a weight Matrix all right weight Matrix size 1.3 gab byte along with this there would be you know other requirement to load your model and to calculate you know uh to to do the calculations so if you consider all those together you can say that okay if you take 4 gab of the memory for your instance 4 gab of RAM memory to remember if you take the 4 GB of RAM memory then that should be more than enough for 1 billion model 1 billion parameter model all right so what I'm going to do here I'm going to select here a model all right so I'm going to select here a model which have a memory of 4 gab so I can see here t2. medium have 4 GB of memory all right so this one I can select apart from this one more thing also you need to pay attention here that currently we are selecting here a model which have only Intel or you can say that the AMD you know architecture that means only CPU all right let me just show you here you know previous architecture which we were using previously all right with that I can explain you how these overall things are going to work so this was overall architecture which we were discussing previously that in our previous videos that this is something which we have now designed here so we hosted our llama and the O Lama on local uh machine and on local machine I had 6 GB of the GPU but on AWS server now you have two option here all right so either you can host this everything on the CPU or you can host this everything on the GPU all right so GPU or CPU but the one thing you have to uh notice here that is in case of the GPU it's going to cost you a lot but in case of the CPU the cost of the host is going to be very small other other than that one more thing you will notice here that in case of the CPU all in case of the CPU the latency is going to be very high all right this is going to be really very high but in case of the GPU the latency is going to be very small so depending on your huge case you can select a particular machine chain type if you are deploying if you are trying to deploy your model into the production where latency matters then you should be selecting their GPU otherwise if you are just doing for the practice and you just want to know that how you can host your application on AWS ec2 Services then you can select their CPU so here if you search here GPU all right so you should be able to select here GPU equal to 1 let's say I select so here you'll be getting all the instances which have a GPU at least one GPU all right so all of these instances have their GPU all right and here it says that the memory you can also you know uh you can also sort this into ascending and descending order I can see here that there is one GPU which is available with the 16 gab of the memory so we can also select this particular model as well which have one GPU other than that what you can do here you can just select one high performance model so I'm going to select here some you know the C type of instances that's the compute intensive instances so I can select here maybe let me just see that C7 a perhaps I can select here depending on the storage and the memory let me just see here that if I can say that this I want this memory equal to 4 GB with this now I can see all of these instances here and then I can select here C7 type of instance this particular instance I can select here previously we had selected there one instance which was like T2 medium instance with the 4GB but now we are selecting here C7 why C7 because this instance is very powerful instance if you just come here and search it it says that this is compute in intensive instance right so these instances are very powerful instances thereafter you can also have C7 I as well somewhere let me just see that here that is here right so you can also select here C7 I what's the difference between C7 a and I so a is AMD instance you s you see it here it says that this is AMD power instance and if you make it here I then this is going to be the Intel powered instances so you can select either AMD or Intel maybe I should be selecting here Intel for a better performance so I select here Intel thereafter I just select this instance type now I have selected here my Ami with the GPU AMD and the P torch I mean GPU Ami and the P torch and now I have selected here 4GB and two CPU with the C7 I large thereafter you also need to select here the key pair so I'm going to create create here new key payer and this key payer I'm going to uh you know I mean name to this one I'm going to give as the chat application thereafter I click on create keyer it is going to download this PM file later on we will be needing this PM file to connect with our server all right therea it says that the security group So currently we are going to select here a default Security Group it is going to allow all the SSH traffic to our you know the system apart from that we have to also make sure that we should be allowing all the HTTP and https Services otherwise we will not be able to access our application so all of these I select here but make sure that you you you know restrict your security depending on your huge cases thereafter we are going to come here it says that how how much memory we are going to uh how much memory we are going to provide to our instance that mean how much memory we are going to give as a storage so I say that I'm going to give here 100 gab of the memory for my machine so thereafter I come here advance details so all of these details I'm going to leave here as it is I'm not going to change anything here thereafter you can get here all the summary so I'm going to launch here only one instance and then it's going to be deep learning one and this is my instance type and it's going to create here a new security group and then 100 gab of the storage volume thereafter I'm just going to click on this launch instance it is going to launch here a brand new instance so we are going to wait for some time you need to come here at ec2 click on here instances which are running currently so I can see here that this instance got created quickly and it is running and status is actually getting checked here so thereafter I can click on here instance ID once you click on instance ID then you can come here at the connect all right then you can just click on that connect thereafter you can select this ec2 instance connect from here itself and then finally you can just connect your connect your machine directly from here so I can just simply click on here connect then it is going to connect with this one all right so it is going to SSH to our ec2 instance from here there are many ways to connect with my ec2 machine if you click on here connect you can see ec2 instance connect session manager SSH client and EC to serial console later on I'll be also showing you that how you can connect here with the SSH client but as of now just consider that we are connecting this with the ac2 instance connect all right so you come here now you can see here you are connected with your machine here now if you just type here a python you should be able to see here that python should come here as the pre-installed software so the python 3.2 is pre-installed here thereafter I can just simply exit from here thereafter you can check that if Kai is installed here seems like Kai is also installed here you can also activate base environment of the K as well it is just taking while to return you know the output after running this K command all right so it says that okay the K is installed there so I can just simply say that here K activate base it should be able to say that okay so it says that first you run K in it I just copy it and then paste it here I should be able to paste it here okay I can just see that the K in it has happened now I need to run here K activate base again it is saying that seems like something is not right with the K and then I can say that K activate but anyway that doesn't matter because the the python has been already installed here now we need to install here stream lit as well so I run here pip install stream lit so that we can host our application so currently we have our application here right so this is our application apart from that couple of things we have to also install here other than the stream lit so this was our stream lit thereafter we have to install Lang chain along with that we have to also install their AMA as well so what I do here I come here to my application here all right so this was our application one more thing I should be uh telling you that here where is this application hosted so this application is hosted at my GitHub repository you can come here at the repo and then there is the ol chatbot so this is our code and this code we will be hosting at the AWS ec2 instance all right so you need to come here at chatbot with the history you just click on here you will be seeing here all the you know instructions which we have to install there so pip install stream lit have been already done there now what else we have to do here we have to install our Lang line chain as well so I just copy and paste it here I WR pip install you know upgrade in quite mode install Lang chain AMA apart from that we have to also install here Lang chain so these are the three Library which we have to install here Lang chin Lang chin AMA and this stream lit after this we have to also install here AMA So currently if you check here ama ama is not installed so we have to come here we have to search here AMA all right then we come here AMA all right thereafter we come here at the download and then we select here Linux then it says that install it with one command so I just simply copy it from here and thereafter I come here at my Linux machine and then I'm just going to paste that here so it is going to install my AMA on my Linux machine so it is going to take a while to install this so we'll wait for some time all right now I can see here that ama is installed here and it also says that Nvidia GPU is also there although we do not have a GPU connected with our machine but yeah so I mean Nvidia GPU is GPU software is also installed there and the is also installed here now if you run here AMA you should be able to see it here now if you type here AMA do help you should be able to get all the type of the help which you have here so like these are the command AMA serve AMA create AMA show AMA run stop pull push all of these now we are going to pull AMA uh we are going to pull here llama from the AMA so we'll be pulling here llama 3.2 1 billion model so I just copy it from here okay so there is run command just to pull it you have to use their pull but for now I'm just going to pull it and then I'm also going to run it on the E2 instance so that we can know that how much time it is going to take to return the output for a particular chat here so I just run it here thereafter see what happens here it is just going to download this llama 3.21 billion model and then it is going to run it here all right so we are just going to wait for some [Music] time okay although I'm not a good singer okay so I'm just waiting to install this da da so our model has been pulled on our system now we are going to chat with our llama model so I just say that hi now we are going to see that how much time it takes here perfect so it is not taking much of the time so as I type here High it is going to return the answer it actually returned the answer although it took some time but that is not that much of the visible time here so basically now you see here a beauty so we have installed our llama with the O Lama on Ubuntu machine and that Ubuntu machine is using their CPU but CPU we are using here high performance CPU C7 I performer so it has 4 GB of the RAM and 2 CPU is getting used there so this is overall system now let's go ahead and exit from the AMA So currently we are outside of the AMA now we have installed all the necessary libraries there now you see there we installed streamlit Lang chain llama and O Lama everything is installed now it is time to host our application there so to host our application what we have to do here we have to actually you know we have to actually copy this one uh I mean the code which we have here currently so this code we have to copy from here to there so one way what we can do here we can just clone this whole repository from this GitHub to there so I can just copy this from here since this is a public repository you can just clone it there as well so what you can do here get the link from here thereafter you come here and then type here get clone and then paste your link here so this is our link here then I'm just going to run it here it is going to clone this repository if you just type here LS it is going to list all the you know the folders here and the files so it says that this is our AMA chatbot let's go ahead go inside our AMA chatbot so I come inside AMA chatbot thereafter if you do here LS you can see here we have this one and we have also this one so we are running actually this one so what I do here again I come here inside our two do chatbot with the history there after you can just see it here you have your file here two. chatbot with the history. py thereafter you can just simply run here stream lit run and therea two do chatbot with the history. py all right thereafter it is going to take a while to run it here now it is running there success y it has provided you here external link you just need to copy this external link if you want to access it from the external world so you just copy this whole link from there and then paste it here now you can see that this should be running there I say here continue on this side okay let's go ahead and just do it again somehow seems like it says that let me just copy and paste it here okay again and then see what happens here it says that we we cannot reach out to this so what could be the reason here okay so there could be multiple reasons because we might not have set our you know the security group for this instance so what I do here I come back here the ac2 and then I come to my instance here so this is our instance which is currently running thereafter we have to come here and check what is the security connected with this I come here at the security thereafter I see here this is our security group and it says that these are the port only opened right so these are the only Port which are open and currently we are trying to access it and on 85 01 Port so what you need to do here you have to come here to your Security Group currently which you have here so this is your Security Group here launch wizard 3 so click on this then it will take you to the security group thereafter you have to thereafter you have to click on here edit inbound rules and in this edit inbound rules you have to add their additional ports as well so I just go here and thereafter I say here custom TCP so instead of that I say that here all traffic and thereafter it's going to take all port and all traffic but that is not always recommended so what you can do here you can say that all TCP all right so all the TCP connection is going to be accepted here and it is going to accept on all Port thereafter you can just simply save it then it says that okay okay thereafter you can just save it here now you see here you have uh saved your security modification now you should come here all right one more thing I should tell you here after you know changing your security security setting it will take a while like 1 to 2 minutes and thereafter this will automatically you know uh this will automatically get attached with your ec2 instance so you don't need to attach it separately now this is attached with your ec2 instance we had refreshed that now you can see it here that at this particular IP address now I can access my uh uh I can access my streamlit application then I say that hi okay then I say here myself H kgp talk I make videos at youtube.com for/ kgp talki let's say and then I just submit it it should be telling you that hello kgp talki and etc etc let's see so it will take a while to generate response because you know that we are hosting our model on the CPU machine so obviously it will take some time to generate this final response so we have to wait for some time maybe like 10 to 20 seconds it might take but overall concept was here to make sure that you learn how to install how to deploy your application on ec2 server on ec2 machine with this I hope that you have learned here that how you can deploy your uh you know application Ama your application with the olama and L chain on AWS ac2 now you can see it here that it says that okay hello kjp talk and etc etc thereafter let's say if I say that what is my name then through the previous chat it should be able to tell that my name is kgp talk so because this application is history enabled application all right now it says that your username on YouTube is kgp talk okay and would you like me to refer you as the kgp or talki all right and uh we can discuss your Channel all right so this is how you can deploy your application on AWS Services apart from that one thing uh I did not show you that how you can connect your ec2 instance uh other than you know through this instance if you want to do that then uh for that purpose what you can do I have I I have a udmi course that udmi course you can actually refer so you come here at Lui Merit thereafter you can come here at the NLP tutorials with the hugging P thereafter here you will get one of my course there is the finetuning llm with hugging face transform for NLP that's the one and there is the another one deployment of machine learning models in production in Python so if you open this you will be seeing that my udmi course here this is one of the highest rated course on the udmi here I discuss so many things and so many ways to deploy your application on AWS ec2 how you can use AWS S3 Services how you can use other AWS services like you can use the Lambda Services you can also use you know you can use their Docker you can use fast API you can use AWS fargate you can use AWS ECR AWS ECS so there are so many ways to use that so the link of this course you can get from the repository that is the NLP tutorial with the hugging face later on I'll be also adding this with my current repository currently which I have here basically our AMA chat so later on you can get from there as well from here as well I hope that this overall uh uh overall tutorial was helpful to you and you must have learned here by now that how you can deploy your chat application on AWS ac2 thereafter one more thing I should be telling you here that let's see if you close this window so this is going to be closed here so to make sure that this keeps running what you can do here you can write here a screen thereafter it's going to create here a new screen and then you can close your window after running the streamlit services here like this streamlit run and then your application here so if you do something like this and then you can press here control A and D together so it is going to De attach that uh that and even though if you close it from here then you can just you know keep seeing your application running here so if you refresh this you should be able to see that your application is still there all right so you can just simply say that hi it should be able to give you the output so this is one concept here another concept let's say if you want to you know restart your machine so if you restart your machine then your IP address will get changed automatically so to fix your IP address what you can do here you can come here and then you can click on elastic ipage in this elastic ipage then you can allocate here one elastic IP address so that you can fix it don't worry about this all of these things are discussed properly in my course where I have talked about the deployment of machine learning models in the production if you want to make sure that you learn every aspect of model deployment in the production using AWS Services then I think you should be watching this course all right I hope that this was helpful to you let's go ahead and do our resource cleanup do not forget about the resource cleanup so to clean up our resource what you can do here you can just select the you know the machine which you want to uh machine which you want to stop thereafter you can just come here and then you can just say that here terminate and delete instances so this is going to terminate and delete your instance completely your application will get lost automatically whatever you have deployed there all right now you see there termination has started now after some time you will see that your application is not accessible at all now you can see it here that you have lost the access to your application so overall the concept which I wanted to make you learn that you learn how to deploy your mod model on the AWS services and I hope that we have achieved that process all right please do like this video And subscribe this Channel and I'll be making more videos on the Lang chain and U Lama and also the llm so that you can get the update directly once you subscribe this channel okay thank you bye-bye take care\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23985"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.youtube.com/watch?v=vCyVVsdKJCg&t=0s', 'start_seconds': 0, 'start_timestamp': '00:00:00'}, page_content=\"hey everyone so today I'm going to show you how you can host your chat application with AMA and Lang chain on AWS services so we'll be using here AWS E2 services to host our application at the server we are using here our Lang chain along with that we are using here AMA and then for this UI we are using here streamlit application so overall building process of this chat application is already covered in our previous videos you can come to my channel kgp talki there is playlist named as local llm tutorial in this local llm tutorial you can find out these two tutorial where I have shown that how you can build your own chat application using AMA and Lang chain now I'm going to show you how you can how you can host your application this application what we are seeing here on local system how you can host this on AWS E2 instance let's go ahead and do that here so the first of all you need to come here at uh AWS console in ec2 window you need to come here if you are at the you know uh at the dashboard here or console home here you need to search ec2 then you need to come here in ec2 dashboard and uh in this e to dashboard you will be seeing something like this then you need to click on here launch instances so we need to launch here one instance so that we can host our application here so what you need to do here you need to first of all give the name for your server so I give name for this as the chat application all right so my instance name is going to be the chat application and therea you need to select here you know the type of Ami which we are going to use here so currently here Amazon Linux 2023 is pre-selected but we are going to select here something else so which type of Ami we are going to select here we are going to select here deep learning Ami so I write here deep learning all right so with this deep learning I can see here all the details so I see here deep learning Nvidia driver Ami GPU py torch so make sure that you select that Ami which have Nvidia GPU and pytorch so I select here this one and along with that I'm going to select here 86 architecture so that it can run on Intel and AMD machines as well thereafter I need to select here the type of instance all right so you need to come here it says that t2. micro so this is very important to select the type of instance currently it is saying the total memory we have here 1 gab of the memory but in our case the type of model which we are going to use here AMA Lama 3.2 all right so this one we are going to use and we will be using here 1 billion parameter model so the 1 billion parameter model size is shown here 1.3 gab all right so this this is just a weight Matrix all right weight Matrix size 1.3 gab byte along with this there would be you know other requirement to load your model and to calculate you know uh to to do the calculations so if you consider all those together you can say that okay if you take 4 gab of the memory for your instance 4 gab of RAM memory to remember if you take the 4 GB of RAM memory then that should be more than enough for 1 billion model 1 billion parameter model all right so what I'm going to do here I'm going to select here a model all right so I'm going to select here a model which have a memory of 4 gab so I can see here t2. medium have 4 GB of memory all right so this one I can select apart from this one more thing also you need to pay attention here that currently we are selecting here a model which have only Intel or you can say that the AMD you know architecture that means only CPU all right let me just show you here you know previous architecture which we were using previously all right with that I can explain you how these overall things are going to work so this was overall architecture which we were discussing previously that in our previous videos that this is something which we have now designed here so we hosted our llama and the O Lama on local uh machine and on local machine I had 6 GB of the GPU but on AWS server now you have two option here all right so either you can host this everything on the CPU or you can host this everything on the GPU all right so GPU or CPU but the one thing you have to uh notice here that is in case of the GPU it's going to cost you a lot but in case of the CPU the cost of the host is going to be very small other other than that one more thing you will notice here that in case of the CPU all in case of the CPU the latency is going to be very high all right this is going to be really very high but in case of the GPU the latency is going to be very small so depending on your huge case you can select a particular machine chain type if you are deploying if you are trying to deploy your model into the production where latency matters then you should be selecting their GPU otherwise if you are just doing for the practice and you just want to know that how you can host your application on AWS ec2 Services then you can select their CPU so here if you search here GPU all right so you should be able to select here GPU equal to 1 let's say I select so here you'll be getting all the instances which have a GPU at least one GPU all right so all of these instances have their GPU all right and here it says that the memory you can also you know uh you can also sort this into ascending and descending order I can see here that there is one GPU which is available with the 16 gab of the memory so we can also select this particular model as well which have one GPU other than that what you can do here you can just select one high performance model so I'm going to select here some you know the C type of instances that's the compute intensive instances so I can select here maybe let me just see that C7 a perhaps I can select here depending on the storage and the memory let me just see here that if I can say that this I want this memory equal to 4 GB with this now I can see all of these instances here and then I can select here C7 type of instance this particular instance I can select here previously we had selected there one instance which was like T2 medium instance with the 4GB but now we are selecting here C7 why C7 because this instance is very powerful instance if you just come here and search it it says that this is compute in intensive instance right so these instances are very powerful instances thereafter you can also have C7 I as well somewhere let me just see that here that is here right so you can also select here C7 I what's the difference between C7 a and I so a is AMD instance you s you see it here it says that this is AMD power instance and if you make it here I then this is going to be the Intel powered instances so you can select either AMD or Intel maybe I should be selecting here Intel for a better performance so I select here Intel thereafter I just select this instance type now I have selected here my Ami with the GPU AMD and the P torch I mean GPU Ami and the P torch and now I have selected here 4GB and two CPU with the C7 I large thereafter you also need to select here the key pair so I'm going to create create here new key payer and this key payer I'm going to uh you know I mean name to this one I'm going to give as the chat application thereafter I click on create keyer it is going to download this PM file later on we will be needing this PM file to connect with our server all right therea it says that the security group So currently we are going to select here a default Security Group it is going to allow all the SSH traffic to our you know the system apart from that we have to also make sure that we should be allowing all the HTTP and https Services otherwise we will not be able to access our application so all of these I select here but make sure that you you you know restrict your security depending on your huge\"),\n",
       " Document(metadata={'source': 'https://www.youtube.com/watch?v=vCyVVsdKJCg&t=600s', 'start_seconds': 600, 'start_timestamp': '00:10:00'}, page_content=\"cases thereafter we are going to come here it says that how how much memory we are going to uh how much memory we are going to provide to our instance that mean how much memory we are going to give as a storage so I say that I'm going to give here 100 gab of the memory for my machine so thereafter I come here advance details so all of these details I'm going to leave here as it is I'm not going to change anything here thereafter you can get here all the summary so I'm going to launch here only one instance and then it's going to be deep learning one and this is my instance type and it's going to create here a new security group and then 100 gab of the storage volume thereafter I'm just going to click on this launch instance it is going to launch here a brand new instance so we are going to wait for some time you need to come here at ec2 click on here instances which are running currently so I can see here that this instance got created quickly and it is running and status is actually getting checked here so thereafter I can click on here instance ID once you click on instance ID then you can come here at the connect all right then you can just click on that connect thereafter you can select this ec2 instance connect from here itself and then finally you can just connect your connect your machine directly from here so I can just simply click on here connect then it is going to connect with this one all right so it is going to SSH to our ec2 instance from here there are many ways to connect with my ec2 machine if you click on here connect you can see ec2 instance connect session manager SSH client and EC to serial console later on I'll be also showing you that how you can connect here with the SSH client but as of now just consider that we are connecting this with the ac2 instance connect all right so you come here now you can see here you are connected with your machine here now if you just type here a python you should be able to see here that python should come here as the pre-installed software so the python 3.2 is pre-installed here thereafter I can just simply exit from here thereafter you can check that if Kai is installed here seems like Kai is also installed here you can also activate base environment of the K as well it is just taking while to return you know the output after running this K command all right so it says that okay the K is installed there so I can just simply say that here K activate base it should be able to say that okay so it says that first you run K in it I just copy it and then paste it here I should be able to paste it here okay I can just see that the K in it has happened now I need to run here K activate base again it is saying that seems like something is not right with the K and then I can say that K activate but anyway that doesn't matter because the the python has been already installed here now we need to install here stream lit as well so I run here pip install stream lit so that we can host our application so currently we have our application here right so this is our application apart from that couple of things we have to also install here other than the stream lit so this was our stream lit thereafter we have to install Lang chain along with that we have to also install their AMA as well so what I do here I come here to my application here all right so this was our application one more thing I should be uh telling you that here where is this application hosted so this application is hosted at my GitHub repository you can come here at the repo and then there is the ol chatbot so this is our code and this code we will be hosting at the AWS ec2 instance all right so you need to come here at chatbot with the history you just click on here you will be seeing here all the you know instructions which we have to install there so pip install stream lit have been already done there now what else we have to do here we have to install our Lang line chain as well so I just copy and paste it here I WR pip install you know upgrade in quite mode install Lang chain AMA apart from that we have to also install here Lang chain so these are the three Library which we have to install here Lang chin Lang chin AMA and this stream lit after this we have to also install here AMA So currently if you check here ama ama is not installed so we have to come here we have to search here AMA all right then we come here AMA all right thereafter we come here at the download and then we select here Linux then it says that install it with one command so I just simply copy it from here and thereafter I come here at my Linux machine and then I'm just going to paste that here so it is going to install my AMA on my Linux machine so it is going to take a while to install this so we'll wait for some time all right now I can see here that ama is installed here and it also says that Nvidia GPU is also there although we do not have a GPU connected with our machine but yeah so I mean Nvidia GPU is GPU software is also installed there and the is also installed here now if you run here AMA you should be able to see it here now if you type here AMA do help you should be able to get all the type of the help which you have here so like these are the command AMA serve AMA create AMA show AMA run stop pull push all of these now we are going to pull AMA uh we are going to pull here llama from the AMA so we'll be pulling here llama 3.2 1 billion model so I just copy it from here okay so there is run command just to pull it you have to use their pull but for now I'm just going to pull it and then I'm also going to run it on the E2 instance so that we can know that how much time it is going to take to return the output for a particular chat here so I just run it here thereafter see what happens here it is just going to download this llama 3.21 billion model and then it is going to run it here all right so we are just going to wait for some [Music] time okay although I'm not a good singer okay so I'm just waiting to install this da da so our model has been pulled on our system now we are going to chat with our llama model so I just say that hi now we are going to see that how much time it takes here perfect so it is not taking much of the time so as I type here High it is going to return the answer it actually returned the answer although it took some time but that is not that much of the visible time here so basically now you see here a beauty so we have installed our llama with the O Lama on Ubuntu machine and that Ubuntu machine is using their CPU but CPU we are using here high performance CPU C7 I performer so it has 4 GB of the RAM and 2 CPU is getting used there so this is overall system now let's go ahead and exit from the AMA So currently we are outside of the AMA now we have installed all the necessary libraries there now you see there we installed streamlit Lang chain llama and O Lama everything is installed now it is time to host our application there so to host our application what we have to do here we have to actually you know we have to actually copy this one uh I mean the code which we have here currently so this code we have to copy from here to there so one way what we can do here we can just clone this whole repository from this GitHub to there so I can just copy this from here since this is a public repository you can just clone it there as well so what you can do here get the link from here thereafter you come here and then type here get clone and then paste your link here so this is our link here then I'm just going to run it here it is going to clone this repository if you just type here LS it is going to list all the you know the folders here and the files so it says that this is our AMA chatbot let's go ahead go inside our AMA chatbot so I come inside AMA chatbot thereafter if you do here LS you can see here we have this one and we have also this one so we are running actually this one so what I do here again I come here inside our two do chatbot with the history there after you can just see it here you have your file here two. chatbot with the history. py thereafter you can just simply run here stream lit run and therea two do chatbot with the history. py all right thereafter it is going to\"),\n",
       " Document(metadata={'source': 'https://www.youtube.com/watch?v=vCyVVsdKJCg&t=1200s', 'start_seconds': 1200, 'start_timestamp': '00:20:00'}, page_content=\"take a while to run it here now it is running there success y it has provided you here external link you just need to copy this external link if you want to access it from the external world so you just copy this whole link from there and then paste it here now you can see that this should be running there I say here continue on this side okay let's go ahead and just do it again somehow seems like it says that let me just copy and paste it here okay again and then see what happens here it says that we we cannot reach out to this so what could be the reason here okay so there could be multiple reasons because we might not have set our you know the security group for this instance so what I do here I come back here the ac2 and then I come to my instance here so this is our instance which is currently running thereafter we have to come here and check what is the security connected with this I come here at the security thereafter I see here this is our security group and it says that these are the port only opened right so these are the only Port which are open and currently we are trying to access it and on 85 01 Port so what you need to do here you have to come here to your Security Group currently which you have here so this is your Security Group here launch wizard 3 so click on this then it will take you to the security group thereafter you have to thereafter you have to click on here edit inbound rules and in this edit inbound rules you have to add their additional ports as well so I just go here and thereafter I say here custom TCP so instead of that I say that here all traffic and thereafter it's going to take all port and all traffic but that is not always recommended so what you can do here you can say that all TCP all right so all the TCP connection is going to be accepted here and it is going to accept on all Port thereafter you can just simply save it then it says that okay okay thereafter you can just save it here now you see here you have uh saved your security modification now you should come here all right one more thing I should tell you here after you know changing your security security setting it will take a while like 1 to 2 minutes and thereafter this will automatically you know uh this will automatically get attached with your ec2 instance so you don't need to attach it separately now this is attached with your ec2 instance we had refreshed that now you can see it here that at this particular IP address now I can access my uh uh I can access my streamlit application then I say that hi okay then I say here myself H kgp talk I make videos at youtube.com for/ kgp talki let's say and then I just submit it it should be telling you that hello kgp talki and etc etc let's see so it will take a while to generate response because you know that we are hosting our model on the CPU machine so obviously it will take some time to generate this final response so we have to wait for some time maybe like 10 to 20 seconds it might take but overall concept was here to make sure that you learn how to install how to deploy your application on ec2 server on ec2 machine with this I hope that you have learned here that how you can deploy your uh you know application Ama your application with the olama and L chain on AWS ac2 now you can see it here that it says that okay hello kjp talk and etc etc thereafter let's say if I say that what is my name then through the previous chat it should be able to tell that my name is kgp talk so because this application is history enabled application all right now it says that your username on YouTube is kgp talk okay and would you like me to refer you as the kgp or talki all right and uh we can discuss your Channel all right so this is how you can deploy your application on AWS Services apart from that one thing uh I did not show you that how you can connect your ec2 instance uh other than you know through this instance if you want to do that then uh for that purpose what you can do I have I I have a udmi course that udmi course you can actually refer so you come here at Lui Merit thereafter you can come here at the NLP tutorials with the hugging P thereafter here you will get one of my course there is the finetuning llm with hugging face transform for NLP that's the one and there is the another one deployment of machine learning models in production in Python so if you open this you will be seeing that my udmi course here this is one of the highest rated course on the udmi here I discuss so many things and so many ways to deploy your application on AWS ec2 how you can use AWS S3 Services how you can use other AWS services like you can use the Lambda Services you can also use you know you can use their Docker you can use fast API you can use AWS fargate you can use AWS ECR AWS ECS so there are so many ways to use that so the link of this course you can get from the repository that is the NLP tutorial with the hugging face later on I'll be also adding this with my current repository currently which I have here basically our AMA chat so later on you can get from there as well from here as well I hope that this overall uh uh overall tutorial was helpful to you and you must have learned here by now that how you can deploy your chat application on AWS ac2 thereafter one more thing I should be telling you here that let's see if you close this window so this is going to be closed here so to make sure that this keeps running what you can do here you can write here a screen thereafter it's going to create here a new screen and then you can close your window after running the streamlit services here like this streamlit run and then your application here so if you do something like this and then you can press here control A and D together so it is going to De attach that uh that and even though if you close it from here then you can just you know keep seeing your application running here so if you refresh this you should be able to see that your application is still there all right so you can just simply say that hi it should be able to give you the output so this is one concept here another concept let's say if you want to you know restart your machine so if you restart your machine then your IP address will get changed automatically so to fix your IP address what you can do here you can come here and then you can click on elastic ipage in this elastic ipage then you can allocate here one elastic IP address so that you can fix it don't worry about this all of these things are discussed properly in my course where I have talked about the deployment of machine learning models in the production if you want to make sure that you learn every aspect of model deployment in the production using AWS Services then I think you should be watching this course all right I hope that this was helpful to you let's go ahead and do our resource cleanup do not forget about the resource cleanup so to clean up our resource what you can do here you can just select the you know the machine which you want to uh machine which you want to stop thereafter you can just come here and then you can just say that here terminate and delete instances so this is going to terminate and delete your instance completely your application will get lost automatically whatever you have deployed there all right now you see there termination has started now after some time you will see that your application is not accessible at all now you can see it here that you have lost the access to your application so overall the concept which I wanted to make you learn that you learn how to deploy your mod model on the AWS services and I hope that we have achieved that process all right please do like this video And subscribe this Channel and I'll be making more videos on the Lang chain and U Lama and also the llm so that you can get the update directly once you subscribe this channel okay thank you bye-bye take care\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read 10 mins video\n",
    "# ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ê¸° ë•Œë¬¸ì— LLMì´ ë§ê°ì„ í•  ìˆ˜ ìˆìŒ.\n",
    "# ì¼ì •ë‹¨ìœ„ë¡œ ì²­í¬ë¥¼ ë‚˜ëˆ  ë¶ˆëŸ¬ì˜¨ë‹¤\n",
    "from langchain_community.document_loaders.youtube import TranscriptFormat\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=vCyVVsdKJCg\"\n",
    "\n",
    "# 600ì´ˆ ì²­í¬\n",
    "loader = YoutubeLoader.from_youtube_url(url, add_video_info=False, transcript_format=TranscriptFormat.CHUNKS, chunk_size_seconds=600)\n",
    "docs = loader.load()\n",
    "\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use LLM to generate youtube keyword\n",
    "from my_scripts import my_llm\n",
    "\n",
    "question = \"\"\"You are an assistant for generating SEO keywords for Youtube.\n",
    "Please generate a list of keywords from the above context.\n",
    "You can use your creativity and correct spelling if it is needed.\"\"\"\n",
    "\n",
    "keywords = []\n",
    "for doc in docs:\n",
    "    kws = my_llm.ask_llm(context=doc.page_content, question=question)\n",
    "    keywords.append(kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a list of SEO keywords based on the provided context:\n",
      "\n",
      "1. AWS E2 services\n",
      "2. Chat application hosting\n",
      "3. AMA and Lang chain\n",
      "4. Local LLM tutorial\n",
      "5. Streamlit application\n",
      "6. AWS console\n",
      "7. EC2 instance launch\n",
      "8. Deep learning Ami\n",
      "9. Nvidia GPU\n",
      "10. Pytorch\n",
      "11. Intel architecture\n",
      "12. C7 A instance\n",
      "13. C7 I instance\n",
      "14. Key pair creation\n",
      "15. Security group configuration\n",
      "\n",
      "Additionally, here are some long-tail keywords that can be helpful for your YouTube video content:\n",
      "\n",
      "1. \"How to host AMA and Lang chain chat application on AWS E2 services\"\n",
      "2. \"Streamlit application development with AMA and Lang chain\"\n",
      "3. \"AWS EC2 instance launch for chat application hosting\"\n",
      "4. \"Deep learning Ami vs Nvidia GPU in chat application hosting\"\n",
      "5. \"Configuring security group for chat application deployment\"\n",
      "\n",
      "Note that you can always adjust and refine this list based on your specific video content and targeting audience., Here's a list of generated keywords based on the provided context:\n",
      "\n",
      "1. AWS EC2 Instance\n",
      "2. Deep Learning Instance\n",
      "3. Security Group\n",
      "4. Storage Volume\n",
      "5. SSH Connection\n",
      "6. EC2 Instance Connect\n",
      "7. Python 3.2\n",
      "8. Streamlit Application\n",
      "9. Lang Chain Library\n",
      "10. AMA (Artificial Mind)\n",
      "\n",
      "Let me know if you'd like me to adjust or expand on this list!, Here are some SEO keywords based on the provided context:\n",
      "\n",
      "1. AWS EC2\n",
      "2. Streamlit Application\n",
      "3. Deploying Chat App on EC2\n",
      "4. AWS S3 Services\n",
      "5. Using Lambda Services\n",
      "6. Fast API\n",
      "7. AWS ECR\n",
      "8. AWS ECS\n",
      "9. Machine Learning Models in Production\n",
      "10. Model Deployment on AWS\n",
      "11. Automating Resource Cleanup\n",
      "12. Securing and Accessing EC2 Instances\n",
      "13. Deploying Model Training Data\n",
      "14. Hugging Face Transformers for NLP\n",
      "15. Streamlit Tutorial for Beginners\n",
      "\n",
      "Additionally, here are some long-tail keywords that can be useful:\n",
      "\n",
      "1. \"How to deploy a chat application on AWS EC2\"\n",
      "2. \"Streamlit application deployment on AWS S3\"\n",
      "3. \"AWS Lambda services for machine learning model deployment\"\n",
      "4. \"Automating resource cleanup on AWS ECS\"\n",
      "5. \"Securing and accessing EC2 instances on AWS ECR\"\n",
      "\n",
      "Please note that these keywords can be further optimized and expanded to include more specific details, such as \"how to deploy a chat application using Streamlit and Fast API\", etc.\n"
     ]
    }
   ],
   "source": [
    "keywords_set = \", \".join(keywords)\n",
    "\n",
    "print(keywords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Above context is the list of relevant keywords for a Youtube video. You need to generate SEO Keywords for it.\"\"\"\n",
    "\n",
    "response = my_llm.ask_llm(keywords_set, question)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
