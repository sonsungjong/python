# pip install transformers>=4.57.0
from transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor, TextIteratorStreamer
import torch
from pathlib import Path
from PIL import Image
from threading import Thread

def main():
    model_id = "Qwen/Qwen3-VL-30B-A3B-Instruct"
    
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # Flash Attention 2 ì„¤ì¹˜: pip install flash-attn --no-build-isolation
    # Flash Attention 2 ì‚¬ìš© (ê¶Œì¥ - ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ)
    try:
        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype="auto",
            attn_implementation="flash_attention_2",
            device_map="auto",
        )
        print("âœ… SUCCESS: Flash Attention 2 í™œì„±í™”")
    except:
        # Flash Attention ì—†ì„ ê²½ìš° ì˜ˆì™¸ì²˜ë¦¬ë¡œ ê¸°ë³¸ ëª¨ë“œ 
        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype="auto",
            device_map="auto"
        )
        print("âš ï¸  WARNING: Flash Attention 2 ì—†ì´ ê¸°ë³¸ ëª¨ë“œë¡œ ë¡œë“œ")
    
    processor = AutoProcessor.from_pretrained(model_id)
    
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")
    
    # ë¬´í•œ ë£¨í”„ë¡œ ì—¬ëŸ¬ ì´ë¯¸ì§€ ì²˜ë¦¬
    while True:
        print("=" * 80)
        image_path_str = input("ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q ë˜ëŠ” quit): ").strip()
        
        # ì¢…ë£Œ ì¡°ê±´
        if image_path_str.lower() in ['q', 'quit', 'exit']:
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        # ë¹ˆ ì…ë ¥ ì²´í¬
        if not image_path_str:
            print("ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
            continue
        
        # ê²½ë¡œ ê²€ì¦
        image_path = Path(image_path_str)
        if not image_path.exists():
            print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}\n")
            continue
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        try:
            image = Image.open(image_path).convert("RGB")
            print(f"âœ… ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ: {image_path}")
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}\n")
            continue
        
        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        prompt = input("í”„ë¡¬í”„íŠ¸ (ê¸°ë³¸: ì´ë¯¸ì§€ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì¤˜): ").strip()
        if not prompt:
            prompt = "ì´ë¯¸ì§€ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì¤˜."
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt},
                ],
            }
        ]
        
        print("\nğŸš€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\n")
        print("=" * 80)
        
        # Preparation for inference
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        # GPUë¡œ ì´ë™
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # Streamer ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
        streamer = TextIteratorStreamer(
            processor.tokenizer,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )
        
        # ìƒì„± íŒŒë¼ë¯¸í„°
        generation_kwargs = {
            **inputs,
            "max_new_tokens": 512,
            "streamer": streamer,
            "do_sample": False,  # greedy decoding
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ìƒì„±
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        print("ğŸ“ ë‹µë³€: ", end="", flush=True)
        for text in streamer:
            print(text, end="", flush=True)
        
        thread.join()
        print("\n" + "=" * 80 + "\n")

if __name__ == "__main__":
    main()
