"""
================================================================================
GGUF ëª¨ë¸ ìƒì„± ê°€ì´ë“œ (ì§ì ‘ ì–‘ìí™” ë²„ì „)
================================================================================

Hugging Face ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ í›„ ì§ì ‘ GGUFë¡œ ë³€í™˜í•˜ê³  ì–‘ìí™”í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
llama-serverë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë”©í•˜ë¯€ë¡œ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.

================================================================================
1. ì„¤ì¹˜ ë° í™˜ê²½ êµ¬ì„±
================================================================================

1-1. llama.cpp ì„¤ì¹˜ ë° ë¹Œë“œ
----------------------------
cd /home/user/source
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir -p build && cd build
cmake .. -DGGML_CUDA=ON -DLLAMA_CURL=OFF
cmake --build . --config Release -j$(nproc)

ë¹Œë“œ ì™„ë£Œ í›„ ë‹¤ìŒ ì‹¤í–‰ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:
- bin/llama-cli       : ëª…ë ¹ì¤„ ì¶”ë¡  ë„êµ¬
- bin/llama-server    : HTTP API ì„œë²„
- bin/llama-quantize  : ëª¨ë¸ ì–‘ìí™” ë„êµ¬

1-2. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
------------------------
pip install requests pillow

================================================================================
2. ëª¨ë¸ ë³€í™˜ ë° ì–‘ìí™” (í•œ ë²ˆë§Œ ì‹¤í–‰)
================================================================================

2-1. Hugging Face ëª¨ë¸ â†’ F16 GGUF ë³€í™˜
---------------------------------------
cd /home/user/source/llama.cpp
python convert_hf_to_gguf.py \
  ~/.cache/huggingface/hub/models--Qwen--Qwen3-VL-30B-A3B-Instruct/snapshots/[í•´ì‹œ]/ \
  --outfile /home/user/models/qwen3-vl-30b-f16.gguf \
  --outtype f16

2-2. F16 â†’ Q4_K_M ì–‘ìí™” (70% ì••ì¶•)
------------------------------------
./build/bin/llama-quantize \
  /home/user/models/qwen3-vl-30b-f16.gguf \
  /home/user/models/qwen3-vl-30b-Q4_K_M.gguf \
  Q4_K_M

ì–‘ìí™” ì˜µì…˜:
- Q4_K_M : 4bit, ê· í˜• (ê¶Œì¥, 70% ì••ì¶•)
- Q5_K_M : 5bit, ê³ í’ˆì§ˆ
- Q8_0   : 8bit, ìµœê³ í’ˆì§ˆ
- Q2_K   : 2bit, ìµœëŒ€ì••ì¶•

ë‹¤ë¥¸ ëª¨ë¸ ì˜ˆì‹œ:
- Qwen3-VL-32B-Thinking
- GPT-OSS-20B
- Exaone-4-32B
ëª¨ë‘ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥

================================================================================
3. ì—¬ëŸ¬ ëª¨ë¸ì„ ë‹¤ë¥¸ í¬íŠ¸ë¡œ ë™ì‹œ ì‹¤í–‰í•˜ê¸°
================================================================================

3-1. ëª¨ë¸ 1: Qwen3-VL-30B (í¬íŠ¸ 8080)
--------------------------------------
/home/user/source/llama.cpp/build/bin/llama-server \
  -m /home/user/models/qwen3-vl-30b-Q4_K_M.gguf \
  -ngl -1 \
  --host 0.0.0.0 \
  --port 8080 \
  -c 4096 \
  > /tmp/llama-server-8080.log 2>&1 &

3-2. ëª¨ë¸ 2: Qwen3-VL-32B-Thinking (í¬íŠ¸ 8081)
-----------------------------------------------
/home/user/source/llama.cpp/build/bin/llama-server \
  -m /home/user/models/qwen3-vl-32b-Q4_K_M.gguf \
  -ngl -1 \
  --host 0.0.0.0 \
  --port 8081 \
  -c 4096 \
  > /tmp/llama-server-8081.log 2>&1 &

3-3. ëª¨ë¸ 3: GPT-OSS-20B (í¬íŠ¸ 8082)
-------------------------------------
/home/user/source/llama.cpp/build/bin/llama-server \
  -m /home/user/models/gpt-oss-20b-Q4_K_M.gguf \
  -ngl -1 \
  --host 0.0.0.0 \
  --port 8082 \
  -c 4096 \
  > /tmp/llama-server-8082.log 2>&1 &

3-4. ì„œë²„ ìƒíƒœ í™•ì¸
-------------------
curl http://localhost:8080/health  # ëª¨ë¸ 1
curl http://localhost:8081/health  # ëª¨ë¸ 2
curl http://localhost:8082/health  # ëª¨ë¸ 3

# ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ì„œë²„ í™•ì¸
ps aux | grep llama-server

# ë¡œê·¸ í™•ì¸
tail -f /tmp/llama-server-8080.log

3-5. ì„œë²„ ì¢…ë£Œ
--------------
# íŠ¹ì • í¬íŠ¸ ì„œë²„ë§Œ ì¢…ë£Œ
pkill -f "llama-server.*8080"

# ëª¨ë“  llama-server ì¢…ë£Œ
pkill llama-server

================================================================================
4. Pythonì—ì„œ ì‚¬ìš©í•˜ê¸°
================================================================================

4-1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„±
---------------------
import requests

response = requests.post("http://localhost:8080/v1/chat/completions", json={
    "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
    "temperature": 0.7,
    "max_tokens": 512
})
print(response.json()["choices"][0]["message"]["content"])

4-2. ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ (Vision ëª¨ë¸)
-----------------------------------
import base64

with open("image.jpg", "rb") as f:
    image_data = base64.b64encode(f.read()).decode("utf-8")

response = requests.post("http://localhost:8080/v1/chat/completions", json={
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì¤˜"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
        ]
    }],
    "max_tokens": 512
})

4-3. ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ ì‚¬ìš©
------------------------
# ê° ëª¨ë¸ì— ë§ëŠ” í¬íŠ¸ë¡œ ìš”ì²­
model1 = requests.post("http://localhost:8080/v1/chat/completions", ...)  # Qwen3-VL-30B-A3B-Instruct
model2 = requests.post("http://localhost:8081/v1/chat/completions", ...)  # Qwen3-VL-32B-Instruct
model3 = requests.post("http://localhost:8082/v1/chat/completions", ...)  # Qwen3-VL-32B-Thinking

================================================================================
5. ì„±ëŠ¥ ìµœì í™” íŒ
================================================================================

5-1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
------------------------
- ì¼ë¶€ ë ˆì´ì–´ë§Œ GPUë¡œ: -ngl 40 (ì „ì²´ ë ˆì´ì–´ì˜ ì¼ë¶€ë§Œ)
- ë” ì‘ì€ ì–‘ìí™”: Q2_K ì‚¬ìš©

5-2. ì†ë„ í–¥ìƒ
--------------
- ë°°ì¹˜ í¬ê¸° ì¦ê°€: -b 2048
- ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ: -c 2048 (ê¸°ë³¸ 4096)
- Flash Attention í™œì„±í™”: --flash-attn

5-3. ë©”ëª¨ë¦¬ ì ˆì•½
----------------
- mmap ì‚¬ìš© (ê¸°ë³¸ í™œì„±í™”ë¨)
- ë‚®ì€ ì–‘ìí™”: Q4_K_M ëŒ€ì‹  Q2_K

================================================================================
6. ë¬¸ì œ í•´ê²°
================================================================================

Q: "unknown model architecture" ì—ëŸ¬
A: llama.cppë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.
   git pull && cmake --build build --config Release

================================================================================
"""
import subprocess
from pathlib import Path
import requests
import json
import sys

LLAMA_CLI = "/home/user/source/llama.cpp/build/bin/llama-cli"
LLAMA_SERVER = "/home/user/source/llama.cpp/build/bin/llama-server"
MODEL_PATH = "/home/user/models/qwen3-vl-30b-Q4_K_M.gguf"
SERVER_URL = "http://localhost:8080"


def check_server_running() -> bool:
    """llama-serverê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸"""
    try:
        response = requests.get(f"{SERVER_URL}/health", timeout=2)
        return response.status_code == 200
    except:
        return False


def start_server():
    """llama-serverë¥¼ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹œì‘"""
    print("\nì„œë²„ë¥¼ ì‹œì‘í•˜ëŠ” ì¤‘... (15ì´ˆ ì •ë„ ì†Œìš”)")
    print(f"ëª…ë ¹ì–´: {LLAMA_SERVER} -m {MODEL_PATH} -ngl -1 --port 8080 -c 4096")
    
    cmd = [
        LLAMA_SERVER,
        "-m", MODEL_PATH,
        "-ngl", "-1",
        "--host", "0.0.0.0",
        "--port", "8080",
        "-c", "4096"
    ]
    
    # ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
    subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    
    # ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    import time
    for i in range(30):
        time.sleep(1)
        if check_server_running():
            print("âœ… ì„œë²„ ì‹œì‘ ì™„ë£Œ!\n")
            return True
        print(f"ëŒ€ê¸° ì¤‘... {i+1}/30")
    
    print("âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
    return False


def extract_text_from_image_api(image_path: str, prompt: str = "ì´ë¯¸ì§€ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì¤˜.") -> str:
    """
    llama-server HTTP APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
    import base64
    with open(image_path, "rb") as f:
        image_data = base64.b64encode(f.read()).decode("utf-8")
    
    # API ìš”ì²­
    payload = {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                ]
            }
        ],
        "temperature": 0.7,
        "max_tokens": 512
    }
    
    response = requests.post(f"{SERVER_URL}/v1/chat/completions", json=payload, timeout=60)
    
    if response.status_code == 200:
        result = response.json()
        return result["choices"][0]["message"]["content"]
    else:
        return f"âŒ ì—ëŸ¬: {response.status_code} - {response.text}"


def extract_text_from_image_cli(image_path: str, prompt: str = "ì´ë¯¸ì§€ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì¤˜.") -> str:
    """
    llama-clië¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì„œë²„ ì—†ì´)
    """
    cmd = [
        LLAMA_CLI,
        "-m", MODEL_PATH,
        "-p", prompt,
        "--image", image_path,
        "-ngl", "-1",
        "-n", "512",
        "--no-cnv",
        "-c", "4096",
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    return result.stdout


def main():
    print("=" * 80)
    print("Qwen3-VL-30B GGUF OCR (ì§ì ‘ ì–‘ìí™” ë²„ì „)")
    print("=" * 80)
    print(f"ëª¨ë¸: {MODEL_PATH}")
    
    # ì„œë²„ ì‹¤í–‰ í™•ì¸
    use_server = False
    if check_server_running():
        print("âœ… llama-serverê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
        use_server = True
    else:
        print("âš ï¸  llama-serverê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        choice = input("ì„œë²„ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¶Œì¥: y): ").strip().lower()
        
        if choice == 'y':
            if start_server():
                use_server = True
            else:
                print("ì„œë²„ ì‹œì‘ ì‹¤íŒ¨. llama-cli ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (ëŠë¦¼).")
        else:
            print("llama-cli ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (ë§¤ë²ˆ 15ì´ˆ ë¡œë”©).")
    
    print("=" * 80 + "\n")
    
    while True:
        # ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥
        image_path_str = input("ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q): ").strip()
        
        # ì¢…ë£Œ ì¡°ê±´
        if image_path_str.lower() in ['q', 'quit', 'exit']:
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not image_path_str:
            print("ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
            continue
        
        image_path = Path(image_path_str)
        
        # ê²½ë¡œ ê²€ì¦
        if not image_path.exists():
            print(f"âŒ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}\n")
            continue
        
        print(f"\nì²˜ë¦¬ ì¤‘: {image_path}")
        print("-" * 80)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if use_server:
            print("ğŸš€ ì„œë²„ APIë¡œ ì¶”ë¡  ì¤‘...")
            output = extract_text_from_image_api(str(image_path))
        else:
            print("â³ llama-clië¡œ ì¶”ë¡  ì¤‘... (15ì´ˆ ì†Œìš”)")
            output = extract_text_from_image_cli(str(image_path))
        
        print("\n" + "=" * 80)
        print("ì¶”ì¶œëœ í…ìŠ¤íŠ¸")
        print("=" * 80)
        print(output)
        print("=" * 80 + "\n")


if __name__ == "__main__":
    main()