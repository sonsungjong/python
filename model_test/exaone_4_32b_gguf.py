from llama_cpp import Llama
import sys

def main():
    model_id = "LGAI-EXAONE/EXAONE-4.0-32B-GGUF"
    # ì–‘ìí™” ì˜µì…˜:
    # - Q4_K_M: í’ˆì§ˆê³¼ ì†ë„ì˜ ê· í˜• (ê¶Œì¥)
    # - Q5_K_M: ë” ë†’ì€ í’ˆì§ˆ
    # - Q6_K: ë§¤ìš° ë†’ì€ í’ˆì§ˆ
    # - Q8_0: ìµœê³  í’ˆì§ˆ
    # - IQ4_XS: ìµœì†Œ í¬ê¸°
    gguf_file = "EXAONE-4.0-32B-Q4_K_M.gguf"
    
    print("=" * 80)
    print("EXAONE 4.0 32B GGUF ëŒ€í™”í˜• ëª¨ë“œ")
    print("=" * 80)
    print(f"ëª¨ë¸ ë¡œë”© ì¤‘... ({gguf_file})")
    print("=" * 80)
    
    # GGUF ëª¨ë¸ ë¡œë“œ
    llm = Llama.from_pretrained(
        repo_id=model_id,
        filename=gguf_file,
        n_gpu_layers=-1,      # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUë¡œ
        n_ctx=8192,           # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (EXAONEì€ 131Kê¹Œì§€ ê°€ëŠ¥í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ê³ ë ¤)
        verbose=False,         # ë¡œë”© ì •ë³´ í‘œì‹œ
        n_threads=8,          # CPU ìŠ¤ë ˆë“œ ìˆ˜
    )
    
    print("\n" + "=" * 80)
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    print("=" * 80)
    print("ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ /bye ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("=" * 80)
    print("\nğŸ’¡ íŒ:")
    print("  - EXAONEì€ í•œêµ­ì–´, ì˜ì–´, ìŠ¤í˜ì¸ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤")
    print("  - Reasoning modeì™€ Non-reasoning modeë¥¼ ì§€ì›í•©ë‹ˆë‹¤")
    print("  - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì—ì„œ 'Reasoning: high'ë¡œ ì¶”ë¡  ë ˆë²¨ ì¡°ì • ê°€ëŠ¥")
    print("=" * 80)
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    messages = []
    
    while True:
        # ì‚¬ìš©ì ì…ë ¥
        user_input = input("\n[You] >>> ")
        
        # ì¢…ë£Œ ëª…ë ¹ ì²´í¬
        if user_input.lower() in ['/bye', 'quit', 'exit', 'q']:
            print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not user_input.strip():
            continue
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": user_input})
        
        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        print("\n[EXAONE] >>> ", end="", flush=True)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ë° ì „ì²´ ì‘ë‹µ ìˆ˜ì§‘
        full_response = ""
        
        try:
            # llama-cpp-pythonì˜ create_chat_completion ì‚¬ìš©
            response = llm.create_chat_completion(
                messages=messages,
                max_tokens=2048,
                stream=True,
                temperature=0.7,
                top_p=0.9,
            )
            
            for chunk in response:
                if 'choices' in chunk and len(chunk['choices']) > 0:
                    delta = chunk['choices'][0].get('delta', {})
                    if 'content' in delta:
                        token = delta['content']
                        print(token, end="", flush=True)
                        full_response += token
            
            print("\n")  # ì¤„ë°”ê¿ˆ
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            # ì¤‘ë‹¨ëœ ê²½ìš°ì—ë„ ì§€ê¸ˆê¹Œì§€ì˜ ì‘ë‹µ ì‚¬ìš©
            if not full_response.strip():
                continue
        
        except Exception as e:
            print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            messages.pop()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì œê±°
            continue
        
        # EXAONEì€ íŠ¹ë³„í•œ êµ¬ë¶„ìê°€ ì—†ìœ¼ë¯€ë¡œ ì „ì²´ë¥¼ ë‹µë³€ìœ¼ë¡œ ì €ì¥
        answer_only = full_response.strip()
        
        # ë‹µë³€ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        if answer_only:
            messages.append({"role": "assistant", "content": answer_only})

if __name__ == "__main__":
    main()
