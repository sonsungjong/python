from llama_cpp import Llama
import sys

def main():
    model_id = "LGAI-EXAONE/EXAONE-4.0-32B-GGUF"
    gguf_file = "EXAONE-4.0-32B-Q4_K_M.gguf"
    
    print("=" * 80)
    print("EXAONE 4.0 32B GGUF ëŒ€í™”í˜• ëª¨ë“œ")
    print("=" * 80)
    print(f"ëª¨ë¸ ë¡œë”© ì¤‘... ({gguf_file})")
    print("=" * 80)
    
    # GGUF ëª¨ë¸ ë¡œë“œ
    try:
        # Jinja2ChatFormatterë¥¼ íŒ¨ì¹˜í•˜ì—¬ continue íƒœê·¸ ì§€ì›
        from llama_cpp import llama_chat_format
        from jinja2 import nodes
        from jinja2.ext import Extension
        
        # Continue íƒœê·¸ë¥¼ ë¬´ì‹œí•˜ëŠ” Jinja2 í™•ì¥
        class ContinueExtension(Extension):
            tags = {'continue'}
            
            def parse(self, parser):
                lineno = next(parser.stream).lineno
                # continueë¥¼ ë¹ˆ Output ë…¸ë“œë¡œ ë³€í™˜
                return nodes.Output([nodes.TemplateData('', lineno=lineno)], lineno=lineno)
        
        # ê¸°ì¡´ Jinja2ChatFormatter.__init__ ë°±ì—…
        original_init = llama_chat_format.Jinja2ChatFormatter.__init__
        
        def patched_init(self, template, *args, **kwargs):
            # Jinja2 í™˜ê²½ì— Continue í™•ì¥ ì¶”ê°€
            from jinja2 import Environment
            
            # kwargsì—ì„œ eos_tokenê³¼ bos_token ì²˜ë¦¬
            self.template = template
            self.eos_token = kwargs.get('eos_token', '')
            self.bos_token = kwargs.get('bos_token', '')
            
            try:
                self._environment = Environment(
                    extensions=[ContinueExtension],
                    trim_blocks=True,
                    lstrip_blocks=True
                )
                self._environment.globals.update({
                    'raise_exception': lambda msg: (_ for _ in ()).throw(Exception(msg)),
                    'strftime_now': lambda fmt: __import__('datetime').datetime.now().strftime(fmt)
                })
                self._template = self._environment.from_string(self.template)
            except Exception as e:
                print(f"âš ï¸  chat_template íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ í¬ë§· ì‚¬ìš©: {e}")
                self._template = None
        
        # __call__ ë©”ì„œë“œë„ íŒ¨ì¹˜
        from llama_cpp.llama_chat_format import ChatFormatterResponse
        
        original_call = llama_chat_format.Jinja2ChatFormatter.__call__
        
        def patched_call(self, *args, **kwargs):
            if not hasattr(self, '_template') or self._template is None:
                # í…œí”Œë¦¿ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í¬ë§· ì‚¬ìš©
                messages = kwargs.get('messages', [])
                prompt = ""
                for msg in messages:
                    role = msg.get('role', '')
                    content = msg.get('content', '')
                    if role == 'system':
                        prompt += f"[|system|]\n{content}[|endofturn|]\n"
                    elif role == 'user':
                        prompt += f"[|user|]\n{content}[|endofturn|]\n"
                    elif role == 'assistant':
                        prompt += f"[|assistant|]\n{content}[|endofturn|]\n"
                prompt += "[|assistant|]\n"
                return ChatFormatterResponse(prompt=prompt, stop=['[|endofturn|]'])
            else:
                # í…œí”Œë¦¿ ì‚¬ìš©
                try:
                    messages = kwargs.get('messages', [])
                    rendered = self._template.render(
                        messages=messages,
                        eos_token=self.eos_token,
                        bos_token=self.bos_token,
                        add_generation_prompt=kwargs.get('add_generation_prompt', True)
                    )
                    return ChatFormatterResponse(prompt=rendered, stop=['[|endofturn|]'])
                except Exception as e:
                    print(f"âš ï¸  í…œí”Œë¦¿ ë Œë”ë§ ì‹¤íŒ¨: {e}")
                    # ê¸°ë³¸ í¬ë§·ìœ¼ë¡œ í´ë°±
                    messages = kwargs.get('messages', [])
                    prompt = ""
                    for msg in messages:
                        role = msg.get('role', '')
                        content = msg.get('content', '')
                        if role == 'system':
                            prompt += f"[|system|]\n{content}[|endofturn|]\n"
                        elif role == 'user':
                            prompt += f"[|user|]\n{content}[|endofturn|]\n"
                        elif role == 'assistant':
                            prompt += f"[|assistant|]\n{content}[|endofturn|]\n"
                    prompt += "[|assistant|]\n"
                    return ChatFormatterResponse(prompt=prompt, stop=['[|endofturn|]'])
        
        # íŒ¨ì¹˜ ì ìš©
        llama_chat_format.Jinja2ChatFormatter.__init__ = patched_init
        llama_chat_format.Jinja2ChatFormatter.__call__ = patched_call
        
        # ì´ì œ ëª¨ë¸ ë¡œë“œ
        llm = Llama.from_pretrained(
            repo_id=model_id,
            filename=gguf_file,
            n_gpu_layers=-1,
            n_ctx=8192,
            verbose=False,
            n_threads=8,
        )
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("\n" + "=" * 80)
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    print("=" * 80)
    print("ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ /bye ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("=" * 80)
    print("\nğŸ’¡ íŒ:")
    print("  - EXAONEì€ í•œêµ­ì–´, ì˜ì–´, ìŠ¤í˜ì¸ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤")
    print("  - ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    print("=" * 80)
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    messages = []
    
    while True:
        # ì‚¬ìš©ì ì…ë ¥
        user_input = input("\n[You] >>> ")
        
        # ì¢…ë£Œ ëª…ë ¹ ì²´í¬
        if user_input.lower() in ['/bye', 'quit', 'exit', 'q']:
            print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not user_input.strip():
            continue
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": user_input})
        
        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        print("\n[EXAONE] >>> ", end="", flush=True)
        
        full_response = ""
        
        try:
            # create_chat_completion ì‚¬ìš©
            response = llm.create_chat_completion(
                messages=messages,
                max_tokens=2048,
                stream=True,
                temperature=0.7,
                top_p=0.9,
            )
            
            for chunk in response:
                if 'choices' in chunk and len(chunk['choices']) > 0:
                    delta = chunk['choices'][0].get('delta', {})
                    if 'content' in delta:
                        token = delta['content']
                        print(token, end="", flush=True)
                        full_response += token
            
            print("\n")  # ì¤„ë°”ê¿ˆ
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if not full_response.strip():
                continue
        
        except Exception as e:
            print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            messages.pop()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì œê±°
            continue
        
        # ë‹µë³€ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        answer_only = full_response.strip()
        if answer_only:
            messages.append({"role": "assistant", "content": answer_only})

if __name__ == "__main__":
    main()
